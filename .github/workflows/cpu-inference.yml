name: cpu-inference

on:
  workflow_dispatch:
  pull_request:
    paths:
      - '.github/workflows/cpu-inference.yml'
      - 'requirements/**'
      - 'deepspeed/__init__.py'
      - 'deepspeed/inference/**'
      - '!deepspeed/inference/v2/**' # exclude v2 dir
      - 'tests/unit/inference/**'
      - '!tests/unit/inference/v2/**' # exclude v2 tests dir
  merge_group:
    branches: [ master ]
  schedule:
        - cron: "0 0 * * 0"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  unit-tests:
    runs-on: [self-hosted, cpu]

    steps:
      - uses: actions/checkout@v3

      - id: setup-venv
        uses: ./.github/workflows/setup-venv

      - name: Install gcc-9
        run: |
          sudo add-apt-repository -u ppa:ubuntu-toolchain-r/test
          sudo apt install -y gcc-9 g++-9
          # set gcc-9 and g++9 to default
          sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 99
          sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 99

      - name: Check gcc version
        run: |
          # Get gcc version
          gcc --version
          g++ --version

      - name: Detect instruction sets on instance
        run: |
          lscpu
          # check whether node has enough memory to load model check point
          cat /proc/meminfo
          # check whether blob has enough space for model check point
          df -k |grep "/blob$"

      - name: Install numactl
        run: |
          sudo apt-get install -y numactl

      - name: Install dependencies
        run: |
          pip install torch
          pip install py-cpuinfo
          # check installed version
          pip list |grep \\\<torch\\\>

      - name: Install transformers
        run: |
          git clone https://github.com/huggingface/transformers
          cd transformers
          git rev-parse --short HEAD
          pip install .

      - name: Install deepspeed
        run: |
          pip install .[dev,1bit,autotuning,inf]
          ds_report

      - name: Python environment check
        run: |
          pip list
          # check whether the environment is properly setup
          python -c "import deepspeed;from deepspeed.accelerator import get_accelerator;print(get_accelerator().device_name());print(get_accelerator().is_available())"

      - name: Download DeepSpeedExamples
        run: |
          git clone https://github.com/Microsoft/DeepSpeedExamples

      - name: AutoTP test (facebook/opt-1.3b)
        run: |
          # modify MODEL to change the model name, other lines are the same
          export MODEL=facebook/opt-1.3b
          cd DeepSpeedExamples/inference/huggingface/text-generation
          deepspeed --num_gpus 2 --bind_cores_to_rank inference-test.py --model $MODEL --dtype bfloat16 --use_meta_tensor
          deepspeed --num_gpus 2 --bind_cores_to_rank ds-hf-compare.py --model $MODEL --dtype bfloat16 --num_inputs 1

      - name: AutoTP test (bigscience/bloom-3b)
        run: |
          # modify MODEL to change the model name, other lines are the same
          export MODEL=bigscience/bloom-3b
          cd DeepSpeedExamples/inference/huggingface/text-generation
          deepspeed --num_gpus 2 --bind_cores_to_rank inference-test.py --model $MODEL --dtype bfloat16 --use_meta_tensor
          deepspeed --num_gpus 2 --bind_cores_to_rank ds-hf-compare.py --model $MODEL --dtype bfloat16 --num_inputs 1

      - name: AutoTP test (baichuan-inc/Baichuan-7B)
        run: |
          # modify MODEL to change the model name, other lines are the same
          export MODEL=baichuan-inc/Baichuan-7B
          export HF_HOME=/blob/hf_home
          echo "directory structure of TRASNSFORMERS_CACHE"
          find $TRANSFORMERS_CACHE
          echo "directory structure of HF_HOME"
          find $HF_HOME
          unset TRANSFORMERS_CACHE
          cd DeepSpeedExamples/inference/huggingface/text-generation
          deepspeed --num_gpus 2 --bind_cores_to_rank inference-test.py --model $MODEL --dtype bfloat16 --trust_remote_code
          deepspeed --num_gpus 2 --bind_cores_to_rank ds-hf-compare.py --model $MODEL --dtype bfloat16 --num_inputs 1 --trust_remote_code

#      - name: AutoTP test (tiiuae/falcon-7b)
#        run: |
#          # modify MODEL to change the model name, other lines are the same
#          export MODEL=tiiuae/falcon-7b
#          cd DeepSpeedExamples/inference/huggingface/text-generation
#          deepspeed --num_gpus 2 --bind_cores_to_rank inference-test.py --model $MODEL --dtype bfloat16 --use_meta_tensor
#          deepspeed --num_gpus 2 --bind_cores_to_rank ds-hf-compare.py --model $MODEL --dtype bfloat16 --num_inputs 1
#
#      - name: AutoTP test (EleutherAI/gpt-j-6b)
#        run: |
#          # modify MODEL to change the model name, other lines are the same
#          export MODEL=EleutherAI/gpt-j-6b
#          cd DeepSpeedExamples/inference/huggingface/text-generation
#          deepspeed --num_gpus 2 --bind_cores_to_rank inference-test.py --model $MODEL --dtype bfloat16 --use_meta_tensor
#          deepspeed --num_gpus 2 --bind_cores_to_rank ds-hf-compare.py --model $MODEL --dtype bfloat16 --num_inputs 1
#
#      - name: AutoTP test (bigcode/starcoder)
#        run: |
#          # modify MODEL to change the model name, other lines are the same
#          export MODEL=bigcode/starcoder
#          cd DeepSpeedExamples/inference/huggingface/text-generation
#          deepspeed --num_gpus 2 --bind_cores_to_rank inference-test.py --model $MODEL --dtype bfloat16 --use_meta_tensor
#          deepspeed --num_gpus 2 --bind_cores_to_rank ds-hf-compare.py --model $MODEL --dtype bfloat16 --num_inputs 1
#
#      - name: AutoTP test (google/flan-t5-xl)
#        run: |
#          # modify MODEL to change the model name, other lines are the same
#          export MODEL=google/flan-t5-xl
#          cd DeepSpeedExamples/inference/huggingface/text-generation
#          deepspeed --num_gpus 2 --bind_cores_to_rank inference-test.py --model $MODEL --dtype bfloat16 --use_meta_tensor
#          deepspeed --num_gpus 2 --bind_cores_to_rank ds-hf-compare.py --model $MODEL --dtype bfloat16 --num_inputs 1
#
#      - name: AutoTP test (mistralai/Mistral-7B-v0.1)
#        run: |
#          # modify MODEL to change the model name, other lines are the same
#          export MODEL=mistralai/Mistral-7B-v0.1
#          cd DeepSpeedExamples/inference/huggingface/text-generation
#          deepspeed --num_gpus 2 --bind_cores_to_rank inference-test.py --model $MODEL --dtype bfloat16 --use_meta_tensor
#          deepspeed --num_gpus 2 --bind_cores_to_rank ds-hf-compare.py --model $MODEL --dtype bfloat16 --num_inputs 1
#
#      - name: AutoTP test (mosaicml/mpt-7b)
#        run: |
#          # modify MODEL to change the model name, other lines are the same
#          export MODEL=mosaicml/mpt-7b
#          cd DeepSpeedExamples/inference/huggingface/text-generation
#          deepspeed --num_gpus 2 --bind_cores_to_rank inference-test.py --model $MODEL --dtype bfloat16 --use_meta_tensor
#          deepspeed --num_gpus 2 --bind_cores_to_rank ds-hf-compare.py --model $MODEL --dtype bfloat16 --num_inputs 1
#
#      - name: AutoTP test (meta-llama/Llama-2-7b-hf)
#        run: |
#          # modify MODEL to change the model name, other lines are the same
#          export MODEL=meta-llama/Llama-2-7b-hf
#          cd DeepSpeedExamples/inference/huggingface/text-generation
#          deepspeed --num_gpus 2 --bind_cores_to_rank inference-test.py --model $MODEL --dtype bfloat16 --use_meta_tensor
#          deepspeed --num_gpus 2 --bind_cores_to_rank ds-hf-compare.py --model $MODEL --dtype bfloat16 --num_inputs 1
#
#      - name: AutoTP test (EleutherAI/gpt-neox-20b)
#        run: |
#          # modify MODEL to change the model name, other lines are the same
#          export MODEL=EleutherAI/gpt-neox-20
#          cd DeepSpeedExamples/inference/huggingface/text-generation
#          deepspeed --num_gpus 2 --bind_cores_to_rank inference-test.py --model $MODEL --dtype bfloat16 --use_meta_tensor
#          deepspeed --num_gpus 2 --bind_cores_to_rank ds-hf-compare.py --model $MODEL --dtype bfloat16 --num_inputs 1
