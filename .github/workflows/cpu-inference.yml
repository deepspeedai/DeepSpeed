name: cpu-inference

on:
  workflow_dispatch:
  pull_request:
    paths:
      - '.github/workflows/cpu-inference.yml'
      - 'requirements/**'
      - 'deepspeed/__init__.py'
      - 'deepspeed/inference/**'
      - '!deepspeed/inference/v2/**' # exclude v2 dir
      - 'tests/unit/inference/**'
      - '!tests/unit/inference/v2/**' # exclude v2 tests dir
  merge_group:
    branches: [ master ]
  schedule:
        - cron: "0 0 * * 0"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  unit-tests:
    runs-on: [self-hosted, cpu, test]

    steps:
      - uses: actions/checkout@v3

      - id: setup-venv
        uses: ./.github/workflows/setup-venv

      - name: Check gcc version
        run: |
          # Get gcc version
          gcc --version
          g++ --version

      - name: Detect instruction sets on instance
        run: |
          lscpu

      - name: Check oneCCL Bindings for PyTorch
        run: |
          pip list

      - name: Install transformers
        run: |
          git clone https://github.com/huggingface/transformers
          cd transformers
          git rev-parse --short HEAD
          pip install .

      - name: Install deepspeed
        run: |
          # check why the host does not have AVX2 support
          pip install .[dev,1bit,autotuning,inf]
          ds_report

      - name: Python environment check
        run: |
          pip list
          export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so.6
          # check whether the environment is properly setup
          python -c "import torch;import intel_extension_for_pytorch as ipex;import oneccl_bindings_for_pytorch;print('done')"
          python -c "import deepspeed;from deepspeed.accelerator import get_accelerator;print(get_accelerator().device_name());print(get_accelerator().is_available())"

      - name: Unit tests
        run: |
          # prep oneCCL for CCLBackend comm ops building
          source oneCCL/build/_install/env/setvars.sh
          export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so.6
          unset TORCH_CUDA_ARCH_LIST # only jit compile for current arch
          cd  tests
          # LOCAL_SIZE=2 enforce CPU to report 2 devices, this helps run the test on github default runner
          LOCAL_SIZE=2 COLUMNS=240 TRANSFORMERS_CACHE=~/tmp/transformers_cache/ TORCH_EXTENSIONS_DIR=./torch-extensions pytest -m 'seq_inference' unit/
          LOCAL_SIZE=2 COLUMNS=240 TRANSFORMERS_CACHE=~/tmp/transformers_cache/ TORCH_EXTENSIONS_DIR=./torch-extensions pytest -m 'inference_ops' -m 'inference' unit/
