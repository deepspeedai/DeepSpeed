<!doctype html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

    <title>deepspeed.pt.deepspeed_lr_schedules API documentation</title>
    <meta name="description" content="Copyright 2019 The Microsoft DeepSpeed Team

Implementation of learning rate schedules.

Taken and m..." />

  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300' rel='stylesheet' type='text/css'>

  <style type="text/css">

* {
  box-sizing: border-box;
}
/*! normalize.css v1.1.1 | MIT License | git.io/normalize */

/* ==========================================================================
   HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
    display: block;
}

/**
 * Correct `inline-block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

audio,
canvas,
video {
    display: inline-block;
    *display: inline;
    *zoom: 1;
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
    display: none;
    height: 0;
}

/**
 * Address styling not present in IE 7/8/9, Firefox 3, and Safari 4.
 * Known issue: no IE 6 support.
 */

[hidden] {
    display: none;
}

/* ==========================================================================
   Base
   ========================================================================== */

/**
 * 1. Prevent system color scheme's background color being used in Firefox, IE,
 *    and Opera.
 * 2. Prevent system color scheme's text color being used in Firefox, IE, and
 *    Opera.
 * 3. Correct text resizing oddly in IE 6/7 when body `font-size` is set using
 *    `em` units.
 * 4. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
    background: #fff; /* 1 */
    color: #000; /* 2 */
    font-size: 100%; /* 3 */
    -webkit-text-size-adjust: 100%; /* 4 */
    -ms-text-size-adjust: 100%; /* 4 */
}

/**
 * Address `font-family` inconsistency between `textarea` and other form
 * elements.
 */

html,
button,
input,
select,
textarea {
    font-family: sans-serif;
}

/**
 * Address margins handled incorrectly in IE 6/7.
 */

body {
    margin: 0;
}

/* ==========================================================================
   Links
   ========================================================================== */

/**
 * Address `outline` inconsistency between Chrome and other browsers.
 */

a:focus {
    outline: thin dotted;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
    outline: 0;
}

/* ==========================================================================
   Typography
   ========================================================================== */

/**
 * Address font sizes and margins set differently in IE 6/7.
 * Address font sizes within `section` and `article` in Firefox 4+, Safari 5,
 * and Chrome.
 */

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}

h2 {
    font-size: 1.5em;
    margin: 0.83em 0;
}

h3 {
    font-size: 1.17em;
    margin: 1em 0;
}

h4 {
    font-size: 1em;
    margin: 1.33em 0;
}

h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}

h6 {
    font-size: 0.67em;
    margin: 2.33em 0;
}

/**
 * Address styling not present in IE 7/8/9, Safari 5, and Chrome.
 */

abbr[title] {
    border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 3+, Safari 4/5, and Chrome.
 */

b,
strong {
    font-weight: bold;
}

blockquote {
    margin: 1em 40px;
}

/**
 * Address styling not present in Safari 5 and Chrome.
 */

dfn {
    font-style: italic;
}

/**
 * Address differences between Firefox and other browsers.
 * Known issue: no IE 6/7 normalization.
 */

hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
}

/**
 * Address styling not present in IE 6/7/8/9.
 */

mark {
    background: #ff0;
    color: #000;
}

/**
 * Address margins set differently in IE 6/7.
 */

p,
pre {
    margin: 1em 0;
}

/**
 * Correct font family set oddly in IE 6, Safari 4/5, and Chrome.
 */

code,
kbd,
pre,
samp {
    font-family: monospace, serif;
    _font-family: 'courier new', monospace;
    font-size: 1em;
}

/**
 * Improve readability of pre-formatted text in all browsers.
 */

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}

/**
 * Address CSS quotes not supported in IE 6/7.
 */

q {
    quotes: none;
}

/**
 * Address `quotes` property not supported in Safari 4.
 */

q:before,
q:after {
    content: '';
    content: none;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
    font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

/* ==========================================================================
   Lists
   ========================================================================== */

/**
 * Address margins set differently in IE 6/7.
 */

dl,
menu,
ol,
ul {
    margin: 1em 0;
}

dd {
    margin: 0 0 0 40px;
}

/**
 * Address paddings set differently in IE 6/7.
 */

menu,
ol,
ul {
    padding: 0 0 0 40px;
}

/**
 * Correct list images handled incorrectly in IE 7.
 */

nav ul,
nav ol {
    list-style: none;
    list-style-image: none;
}

/* ==========================================================================
   Embedded content
   ========================================================================== */

/**
 * 1. Remove border when inside `a` element in IE 6/7/8/9 and Firefox 3.
 * 2. Improve image quality when scaled in IE 7.
 */

img {
    border: 0; /* 1 */
    -ms-interpolation-mode: bicubic; /* 2 */
}

/**
 * Correct overflow displayed oddly in IE 9.
 */

svg:not(:root) {
    overflow: hidden;
}

/* ==========================================================================
   Figures
   ========================================================================== */

/**
 * Address margin not present in IE 6/7/8/9, Safari 5, and Opera 11.
 */

figure {
    margin: 0;
}

/* ==========================================================================
   Forms
   ========================================================================== */

/**
 * Correct margin displayed oddly in IE 6/7.
 */

form {
    margin: 0;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct color not being inherited in IE 6/7/8/9.
 * 2. Correct text not wrapping in Firefox 3.
 * 3. Correct alignment displayed oddly in IE 6/7.
 */

legend {
    border: 0; /* 1 */
    padding: 0;
    white-space: normal; /* 2 */
    *margin-left: -7px; /* 3 */
}

/**
 * 1. Correct font size not being inherited in all browsers.
 * 2. Address margins set differently in IE 6/7, Firefox 3+, Safari 5,
 *    and Chrome.
 * 3. Improve appearance and consistency in all browsers.
 */

button,
input,
select,
textarea {
    font-size: 100%; /* 1 */
    margin: 0; /* 2 */
    vertical-align: baseline; /* 3 */
    *vertical-align: middle; /* 3 */
}

/**
 * Address Firefox 3+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

button,
input {
    line-height: normal;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Chrome, Safari 5+, and IE 6+.
 * Correct `select` style inheritance in Firefox 4+ and Opera.
 */

button,
select {
    text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 * 4. Remove inner spacing in IE 7 without affecting normal text inputs.
 *    Known issue: inner spacing remains in IE 6.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
    *overflow: visible;  /* 4 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
    cursor: default;
}

/**
 * 1. Address box sizing set to content-box in IE 8/9.
 * 2. Remove excess padding in IE 8/9.
 * 3. Remove excess padding in IE 7.
 *    Known issue: excess padding remains in IE 6.
 */

input[type="checkbox"],
input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
    *height: 13px; /* 3 */
    *width: 13px; /* 3 */
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari 5 and Chrome
 * on OS X.
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
}

/**
 * Remove inner padding and border in Firefox 3+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0;
}

/**
 * 1. Remove default vertical scrollbar in IE 6/7/8/9.
 * 2. Improve readability and alignment in all browsers.
 */

textarea {
    overflow: auto; /* 1 */
    vertical-align: top; /* 2 */
}

/* ==========================================================================
   Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
    border-collapse: collapse;
    border-spacing: 0;
}

  </style>

  <style type="text/css">

  html, body {
    margin: 0;
    padding: 0;
    min-height: 100%;
  }
  body {
    background: #fff;
    font-family: "Source Sans Pro", "Helvetica Neueue", Helvetica, sans;
    font-weight: 300;
    font-size: 16px;
    line-height: 1.6em;
  }
  #content {
    width: 70%;
    max-width: 850px;
    float: left;
    padding: 30px 60px;
    border-left: 1px solid #ddd;
  }
  #sidebar {
    width: 25%;
    float: left;
    padding: 30px;
    overflow: hidden;
  }
  #nav {
    font-size: 130%;
    margin: 0 0 15px 0;
  }

  #top {
    display: block;
    position: fixed;
    bottom: 5px;
    left: 5px;
    font-size: .85em;
    text-transform: uppercase;
  }

  #footer {
    font-size: .75em;
    padding: 5px 30px;
    border-top: 1px solid #ddd;
    text-align: right;
  }
    #footer p {
      margin: 0 0 0 30px;
      display: inline-block;
    }

  h1, h2, h3, h4, h5 {
    font-weight: 300;
  }
  h1 {
    font-size: 2.5em;
    line-height: 1.1em;
    margin: 0 0 .50em 0;
  }

  h2 {
    font-size: 1.75em;
    margin: 1em 0 .50em 0;
  }

  h3 {
    margin: 25px 0 10px 0;
  }

  h4 {
    margin: 0;
    font-size: 105%;
  }

  a {
    color: #058;
    text-decoration: none;
    transition: color .3s ease-in-out;
  }

  a:hover {
    color: #e08524;
    transition: color .3s ease-in-out;
  }

  pre, code, .mono, .name {
    font-family: "Ubuntu Mono", "Cousine", "DejaVu Sans Mono", monospace;
  }

  .title .name {
    font-weight: bold;
  }
  .section-title {
    margin-top: 2em;
  }
  .ident {
    color: #900;
  }

  code {
    background: #f9f9f9;
  }

  pre {
    background: #fefefe;
    border: 1px solid #ddd;
    box-shadow: 2px 2px 0 #f3f3f3;
    margin: 0 30px;
    padding: 15px 30px;
  }

  .codehilite {
    margin: 0 30px 10px 30px;
  }

    .codehilite pre {
      margin: 0;
    }
    .codehilite .err { background: #ff3300; color: #fff !important; }

  table#module-list {
    font-size: 110%;
  }

    table#module-list tr td:first-child {
      padding-right: 10px;
      white-space: nowrap;
    }

    table#module-list td {
      vertical-align: top;
      padding-bottom: 8px;
    }

      table#module-list td p {
        margin: 0 0 7px 0;
      }

  .def {
    display: table;
  }

    .def p {
      display: table-cell;
      vertical-align: top;
      text-align: left;
    }

    .def p:first-child {
      white-space: nowrap;
    }

    .def p:last-child {
      width: 100%;
    }


  #index {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }
    ul#index .class_name {
      /* font-size: 110%; */
      font-weight: bold;
    }
    #index ul {
      margin: 0;
    }

  .item {
    margin: 0 0 15px 0;
  }

    .item .class {
      margin: 0 0 25px 30px;
    }

      .item .class ul.class_list {
        margin: 0 0 20px 0;
      }

    .item .name {
      background: #fafafa;
      margin: 0;
      font-weight: bold;
      padding: 5px 10px;
      border-radius: 3px;
      display: inline-block;
      min-width: 40%;
    }
      .item .name:hover {
        background: #f6f6f6;
      }

    .item .empty_desc {
      margin: 0 0 5px 0;
      padding: 0;
    }

    .item .inheritance {
      margin: 3px 0 0 30px;
    }

    .item .inherited {
      color: #666;
    }

    .item .desc {
      padding: 0 8px;
      margin: 0;
    }

      .item .desc p {
        margin: 0 0 10px 0;
      }

    .source_cont {
      margin: 0;
      padding: 0;
    }

    .source_link a {
      background: #ffc300;
      font-weight: 400;
      font-size: .75em;
      text-transform: uppercase;
      color: #fff;
      text-shadow: 1px 1px 0 #f4b700;

      padding: 3px 8px;
      border-radius: 2px;
      transition: background .3s ease-in-out;
    }
      .source_link a:hover {
        background: #FF7200;
        text-shadow: none;
        transition: background .3s ease-in-out;
      }

    .source {
      display: none;
      max-height: 600px;
      overflow-y: scroll;
      margin-bottom: 15px;
    }

      .source .codehilite {
        margin: 0;
      }

  .desc h1, .desc h2, .desc h3 {
    font-size: 100% !important;
  }
  .clear {
    clear: both;
  }

  @media all and (max-width: 950px) {
    #sidebar {
      width: 35%;
    }
    #content {
      width: 65%;
    }
  }
  @media all and (max-width: 650px) {
    #top {
      display: none;
    }
    #sidebar {
      float: none;
      width: auto;
    }
    #content {
      float: none;
      width: auto;
      padding: 30px;
    }

    #index ul {
      padding: 0;
      margin-bottom: 15px;
    }
    #index ul li {
      display: inline-block;
      margin-right: 30px;
    }
    #footer {
      text-align: left;
    }
    #footer p {
      display: block;
      margin: inherit;
    }
  }

  /*****************************/

  </style>

  <style type="text/css">
  .codehilite .hll { background-color: #ffffcc }
.codehilite  { background: #f8f8f8; }
.codehilite .c { color: #408080; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #FF0000 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666666 } /* Operator */
.codehilite .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #BC7A00 } /* Comment.Preproc */
.codehilite .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #408080; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #408080; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .gr { color: #FF0000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #00A000 } /* Generic.Inserted */
.codehilite .go { color: #888888 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #0044DD } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #7D9029 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.codehilite .no { color: #880000 } /* Name.Constant */
.codehilite .nd { color: #AA22FF } /* Name.Decorator */
.codehilite .ni { color: #999999; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #0000FF } /* Name.Function */
.codehilite .nl { color: #A0A000 } /* Name.Label */
.codehilite .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #bbbbbb } /* Text.Whitespace */
.codehilite .mb { color: #666666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666666 } /* Literal.Number.Float */
.codehilite .mh { color: #666666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #BB6688 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #0000FF } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>

  <style type="text/css">

/* ==========================================================================
   EXAMPLE Media Queries for Responsive Design.
   These examples override the primary ('mobile first') styles.
   Modify as content requires.
   ========================================================================== */

@media only screen and (min-width: 35em) {
    /* Style adjustments for viewports that meet the condition */
}

@media print,
       (-o-min-device-pixel-ratio: 5/4),
       (-webkit-min-device-pixel-ratio: 1.25),
       (min-resolution: 120dpi) {
    /* Style adjustments for high resolution devices */
}

/* ==========================================================================
   Print styles.
   Inlined to avoid required HTTP connection: h5bp.com/r
   ========================================================================== */

@media print {
    * {
        background: transparent !important;
        color: #000 !important; /* Black prints faster: h5bp.com/s */
        box-shadow: none !important;
        text-shadow: none !important;
    }

    a,
    a:visited {
        text-decoration: underline;
    }

    a[href]:after {
        content: " (" attr(href) ")";
    }

    abbr[title]:after {
        content: " (" attr(title) ")";
    }

    /*
     * Don't show links for images, or javascript/internal links
     */

    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }

    pre,
    blockquote {
        border: 1px solid #999;
        page-break-inside: avoid;
    }

    thead {
        display: table-header-group; /* h5bp.com/t */
    }

    tr,
    img {
        page-break-inside: avoid;
    }

    img {
        max-width: 100% !important;
    }

    @page {
        margin: 0.5cm;
    }

    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }

    h2,
    h3 {
        page-break-after: avoid;
    }
}

  </style>

  <script type="text/javascript">
  function toggle(id, $link) {
    $node = document.getElementById(id);
    if (!$node)
    return;
    if (!$node.style.display || $node.style.display == 'none') {
    $node.style.display = 'block';
    $link.innerHTML = 'Hide source &nequiv;';
    } else {
    $node.style.display = 'none';
    $link.innerHTML = 'Show source &equiv;';
    }
  }
  </script>
</head>
<body>
<a href="#" id="top">Top</a>

<div id="container">


  <div id="sidebar">
    <h1>Index</h1>
    <ul id="index">
    <li class="set"><h3><a href="#header-variables">Module variables</a></h3>

  <ul>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.ALLGATHER_SIZE">ALLGATHER_SIZE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.ALLGATHER_SIZE_DEFAULT">ALLGATHER_SIZE_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.CYCLE_FIRST_STAIR_COUNT">CYCLE_FIRST_STAIR_COUNT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.CYCLE_FIRST_STEP_SIZE">CYCLE_FIRST_STEP_SIZE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.CYCLE_MAX_LR">CYCLE_MAX_LR</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.CYCLE_MAX_MOM">CYCLE_MAX_MOM</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.CYCLE_MIN_LR">CYCLE_MIN_LR</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.CYCLE_MIN_MOM">CYCLE_MIN_MOM</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.CYCLE_SECOND_STAIR_COUNT">CYCLE_SECOND_STAIR_COUNT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.CYCLE_SECOND_STEP_SIZE">CYCLE_SECOND_STEP_SIZE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.DECAY_LR_RATE">DECAY_LR_RATE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.DECAY_MOM_RATE">DECAY_MOM_RATE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.DECAY_STEP_SIZE">DECAY_STEP_SIZE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.DISABLE_ALLGATHER">DISABLE_ALLGATHER</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.DISABLE_ALLGATHER_DEFAULT">DISABLE_ALLGATHER_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.DISABLE_ALLGATHER_FORMAT">DISABLE_ALLGATHER_FORMAT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.DUMP_STATE">DUMP_STATE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.DUMP_STATE_DEFAULT">DUMP_STATE_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.DUMP_STATE_FORMAT">DUMP_STATE_FORMAT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.EDGE_VALUE">EDGE_VALUE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16">FP16</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16_ENABLED">FP16_ENABLED</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16_ENABLED_DEFAULT">FP16_ENABLED_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16_FORMAT">FP16_FORMAT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16_HYSTERESIS">FP16_HYSTERESIS</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16_HYSTERESIS_DEFAULT">FP16_HYSTERESIS_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16_INITIAL_SCALE_POWER">FP16_INITIAL_SCALE_POWER</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16_INITIAL_SCALE_POWER_DEFAULT">FP16_INITIAL_SCALE_POWER_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16_LOSS_SCALE">FP16_LOSS_SCALE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16_LOSS_SCALE_DEFAULT">FP16_LOSS_SCALE_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16_LOSS_SCALE_WINDOW">FP16_LOSS_SCALE_WINDOW</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16_LOSS_SCALE_WINDOW_DEFAULT">FP16_LOSS_SCALE_WINDOW_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16_MIN_LOSS_SCALE">FP16_MIN_LOSS_SCALE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP16_MIN_LOSS_SCALE_DEFAULT">FP16_MIN_LOSS_SCALE_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP32_ALLREDUCE">FP32_ALLREDUCE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP32_ALLREDUCE_DEFAULT">FP32_ALLREDUCE_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.FP32_ALLREDUCE_FORMAT">FP32_ALLREDUCE_FORMAT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.GRADIENT_ACCUMULATION_FORMAT">GRADIENT_ACCUMULATION_FORMAT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.GRADIENT_ACCUMULATION_STEPS">GRADIENT_ACCUMULATION_STEPS</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.GRADIENT_ACCUMULATION_STEPS_DEFAULT">GRADIENT_ACCUMULATION_STEPS_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.GRADIENT_CLIPPING">GRADIENT_CLIPPING</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.GRADIENT_CLIPPING_DEFAULT">GRADIENT_CLIPPING_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.GRADIENT_CLIPPING_FORMAT">GRADIENT_CLIPPING_FORMAT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.LR_RANGE_TEST">LR_RANGE_TEST</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.LR_RANGE_TEST_MIN_LR">LR_RANGE_TEST_MIN_LR</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.LR_RANGE_TEST_STAIRCASE">LR_RANGE_TEST_STAIRCASE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.LR_RANGE_TEST_STEP_RATE">LR_RANGE_TEST_STEP_RATE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.LR_RANGE_TEST_STEP_SIZE">LR_RANGE_TEST_STEP_SIZE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.LR_SCHEDULE">LR_SCHEDULE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.MAX_GRAD_NORM">MAX_GRAD_NORM</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.MID_VALUE">MID_VALUE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.ONE_CYCLE">ONE_CYCLE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.OPTIMIZER">OPTIMIZER</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.OPTIMIZER_PARAMS">OPTIMIZER_PARAMS</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.OPTIMIZER_TYPE_DEFAULT">OPTIMIZER_TYPE_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.PRESCALE_GRADIENTS">PRESCALE_GRADIENTS</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.PRESCALE_GRADIENTS_DEFAULT">PRESCALE_GRADIENTS_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.PRESCALE_GRADIENTS_FORMAT">PRESCALE_GRADIENTS_FORMAT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.ROUTE_ENCODE">ROUTE_ENCODE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.ROUTE_EVAL">ROUTE_EVAL</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.ROUTE_PREDICT">ROUTE_PREDICT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.ROUTE_TRAIN">ROUTE_TRAIN</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.SCHEDULER">SCHEDULER</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.SCHEDULER_PARAMS">SCHEDULER_PARAMS</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.SCHEDULER_TYPE_DEFAULT">SCHEDULER_TYPE_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.SPARSE_GRADIENTS">SPARSE_GRADIENTS</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.SPARSE_GRADIENTS_DEFAULT">SPARSE_GRADIENTS_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.STEPS_PER_PRINT">STEPS_PER_PRINT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.STEPS_PER_PRINT_DEFAULT">STEPS_PER_PRINT_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD">TENSORBOARD</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_ENABLED">TENSORBOARD_ENABLED</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_ENABLED_DEFAULT">TENSORBOARD_ENABLED_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_FORMAT">TENSORBOARD_FORMAT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_JOB_NAME">TENSORBOARD_JOB_NAME</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_JOB_NAME_DEFAULT">TENSORBOARD_JOB_NAME_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_OUTPUT_PATH">TENSORBOARD_OUTPUT_PATH</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_OUTPUT_PATH_DEFAULT">TENSORBOARD_OUTPUT_PATH_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.TRAIN_BATCH_SIZE">TRAIN_BATCH_SIZE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.TRAIN_BATCH_SIZE_DEFAULT">TRAIN_BATCH_SIZE_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.TRAIN_MICRO_BATCH_SIZE_PER_GPU">TRAIN_MICRO_BATCH_SIZE_PER_GPU</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.TRAIN_MICRO_BATCH_SIZE_PER_GPU_DEFAULT">TRAIN_MICRO_BATCH_SIZE_PER_GPU_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.TYPE">TYPE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.VALID_LR_SCHEDULES">VALID_LR_SCHEDULES</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.VOCABULARY_SIZE">VOCABULARY_SIZE</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.VOCABULARY_SIZE_DEFAULT">VOCABULARY_SIZE_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.VOCABULARY_SIZE_FORMAT">VOCABULARY_SIZE_FORMAT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.WALL_CLOCK_BREAKDOWN">WALL_CLOCK_BREAKDOWN</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.WALL_CLOCK_BREAKDOWN_DEFAULT">WALL_CLOCK_BREAKDOWN_DEFAULT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.WALL_CLOCK_BREAKDOWN_FORMAT">WALL_CLOCK_BREAKDOWN_FORMAT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.WARMUP_LR">WARMUP_LR</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.WARMUP_MAX_LR">WARMUP_MAX_LR</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.WARMUP_MIN_LR">WARMUP_MIN_LR</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.WARMUP_NUM_STEPS">WARMUP_NUM_STEPS</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.ZERO_FORMAT">ZERO_FORMAT</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.ZERO_OPTIMIZATION">ZERO_OPTIMIZATION</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.ZERO_OPTIMIZATION_DEFAULT">ZERO_OPTIMIZATION_DEFAULT</a></li>
  </ul>

    </li>

    <li class="set"><h3><a href="#header-functions">Functions</a></h3>

  <ul>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.add_tuning_arguments">add_tuning_arguments</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.get_config_from_args">get_config_from_args</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.get_lr_from_config">get_lr_from_config</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.override_1cycle_params">override_1cycle_params</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.override_lr_range_test_params">override_lr_range_test_params</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.override_params">override_params</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.override_warmupLR_params">override_warmupLR_params</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.parse_arguments">parse_arguments</a></li>
  </ul>

    </li>

    <li class="set"><h3><a href="#header-classes">Classes</a></h3>
      <ul>
        <li class="mono">
        <span class="class_name"><a href="#deepspeed.pt.deepspeed_lr_schedules.LRRangeTest">LRRangeTest</a></span>


  <ul>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.__init__">__init__</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.get_lr">get_lr</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.load_state_dict">load_state_dict</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.state_dict">state_dict</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.step">step</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#deepspeed.pt.deepspeed_lr_schedules.OneCycle">OneCycle</a></span>


  <ul>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.OneCycle.__init__">__init__</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.OneCycle.get_lr">get_lr</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.OneCycle.load_state_dict">load_state_dict</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.OneCycle.state_dict">state_dict</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.OneCycle.step">step</a></li>
  </ul>

        </li>
        <li class="mono">
        <span class="class_name"><a href="#deepspeed.pt.deepspeed_lr_schedules.WarmupLR">WarmupLR</a></span>


  <ul>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.WarmupLR.__init__">__init__</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.WarmupLR.get_lr">get_lr</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.WarmupLR.load_state_dict">load_state_dict</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.WarmupLR.state_dict">state_dict</a></li>
    <li class="mono"><a href="#deepspeed.pt.deepspeed_lr_schedules.WarmupLR.step">step</a></li>
  </ul>

        </li>
      </ul>
    </li>

    </ul>
  </div>

    <article id="content">






  <header id="section-intro">
  <h1 class="title"><span class="name">deepspeed.pt.deepspeed_lr_schedules</span> module</h1>
  <p>Copyright 2019 The Microsoft DeepSpeed Team</p>
<p>Implementation of learning rate schedules.</p>
<p>Taken and modified from PyTorch v1.0.1 source
https://github.com/pytorch/pytorch/blob/v1.1.0/torch/optim/lr_scheduler.py</p>

  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules" class="source">
    <div class="codehilite"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Copyright 2019 The Microsoft DeepSpeed Team</span>

<span class="sd">Implementation of learning rate schedules.</span>

<span class="sd">Taken and modified from PyTorch v1.0.1 source</span>
<span class="sd">https://github.com/pytorch/pytorch/blob/v1.1.0/torch/optim/lr_scheduler.py</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">deepspeed.pt.deepspeed_constants</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">LR_SCHEDULE</span> <span class="o">=</span> <span class="s1">&#39;lr_schedule&#39;</span>
<span class="n">LR_RANGE_TEST</span> <span class="o">=</span> <span class="s1">&#39;LRRangeTest&#39;</span>
<span class="n">ONE_CYCLE</span> <span class="o">=</span> <span class="s1">&#39;OneCycle&#39;</span>
<span class="n">WARMUP_LR</span> <span class="o">=</span> <span class="s1">&#39;WarmupLR&#39;</span>
<span class="n">VALID_LR_SCHEDULES</span> <span class="o">=</span> <span class="p">[</span><span class="n">LR_RANGE_TEST</span><span class="p">,</span> <span class="n">ONE_CYCLE</span><span class="p">,</span> <span class="n">WARMUP_LR</span><span class="p">]</span>

<span class="n">LR_RANGE_TEST_MIN_LR</span> <span class="o">=</span> <span class="s1">&#39;lr_range_test_min_lr&#39;</span>
<span class="n">LR_RANGE_TEST_STEP_RATE</span> <span class="o">=</span> <span class="s1">&#39;lr_range_test_step_rate&#39;</span>
<span class="n">LR_RANGE_TEST_STEP_SIZE</span> <span class="o">=</span> <span class="s1">&#39;lr_range_test_step_size&#39;</span>
<span class="n">LR_RANGE_TEST_STAIRCASE</span> <span class="o">=</span> <span class="s1">&#39;lr_range_test_staircase&#39;</span>

<span class="n">EDGE_VALUE</span> <span class="o">=</span> <span class="s1">&#39;edge_value&#39;</span>
<span class="n">MID_VALUE</span> <span class="o">=</span> <span class="s1">&#39;mid_value&#39;</span>

<span class="n">CYCLE_FIRST_STEP_SIZE</span> <span class="o">=</span> <span class="s1">&#39;cycle_first_step_size&#39;</span>
<span class="n">CYCLE_FIRST_STAIR_COUNT</span> <span class="o">=</span> <span class="s1">&#39;cycle_first_stair_count&#39;</span>
<span class="n">CYCLE_SECOND_STEP_SIZE</span> <span class="o">=</span> <span class="s1">&#39;cycle_second_step_size&#39;</span>
<span class="n">CYCLE_SECOND_STAIR_COUNT</span> <span class="o">=</span> <span class="s1">&#39;cycle_second_stair_count&#39;</span>
<span class="n">DECAY_STEP_SIZE</span> <span class="o">=</span> <span class="s1">&#39;decay_step_size&#39;</span>

<span class="n">CYCLE_MIN_LR</span> <span class="o">=</span> <span class="s1">&#39;cycle_min_lr&#39;</span>
<span class="n">CYCLE_MAX_LR</span> <span class="o">=</span> <span class="s1">&#39;cycle_max_lr&#39;</span>
<span class="n">DECAY_LR_RATE</span> <span class="o">=</span> <span class="s1">&#39;decay_lr_rate&#39;</span>

<span class="n">CYCLE_MIN_MOM</span> <span class="o">=</span> <span class="s1">&#39;cycle_min_mom&#39;</span>
<span class="n">CYCLE_MAX_MOM</span> <span class="o">=</span> <span class="s1">&#39;cycle_max_mom&#39;</span>
<span class="n">DECAY_MOM_RATE</span> <span class="o">=</span> <span class="s1">&#39;decay_mom_rate&#39;</span>

<span class="n">WARMUP_MIN_LR</span> <span class="o">=</span> <span class="s1">&#39;warmup_min_lr&#39;</span>
<span class="n">WARMUP_MAX_LR</span> <span class="o">=</span> <span class="s1">&#39;warmup_max_lr&#39;</span>
<span class="n">WARMUP_NUM_STEPS</span> <span class="o">=</span> <span class="s1">&#39;warmup_num_steps&#39;</span>


<span class="k">def</span> <span class="nf">add_tuning_arguments</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span><span class="s1">&#39;Convergence Tuning&#39;</span><span class="p">,</span>
                                      <span class="s1">&#39;Convergence tuning configurations&#39;</span><span class="p">)</span>

    <span class="c1"># LR scheduler</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr_schedule&#39;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;LR schedule for training.&#39;</span><span class="p">)</span>

    <span class="c1"># Learning rate range test</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--lr_range_test_min_lr&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Starting lr value.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--lr_range_test_step_rate&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;scaling rate for LR range test.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--lr_range_test_step_size&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;training steps per LR change.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--lr_range_test_staircase&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;use staircase scaling for LR range test.&#39;</span><span class="p">)</span>

    <span class="c1"># OneCycle schedule</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_first_step_size&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;size of first step of 1Cycle schedule (training steps).&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_first_stair_count&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;first stair count for 1Cycle schedule.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--cycle_second_step_size&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;size of second step of 1Cycle schedule (default first_step_size).&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_second_stair_count&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;second stair count for 1Cycle schedule.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--decay_step_size&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;size of intervals for applying post cycle decay (training steps).&#39;</span><span class="p">)</span>

    <span class="c1"># 1Cycle LR</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_min_lr&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;1Cycle LR lower bound.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_max_lr&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;1Cycle LR upper bound.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--decay_lr_rate&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;post cycle LR decay rate.&#39;</span><span class="p">)</span>

    <span class="c1"># 1Cycle Momentum</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--cycle_momentum&#39;</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="n">action</span><span class="o">=</span><span class="s1">&#39;store_true&#39;</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Enable 1Cycle momentum schedule.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_min_mom&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;1Cycle momentum lower bound.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_max_mom&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;1Cycle momentum upper bound.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--decay_mom_rate&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;post cycle momentum decay rate.&#39;</span><span class="p">)</span>

    <span class="c1"># Warmup LR</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--warmup_min_lr&#39;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;WarmupLR minimum/initial LR value&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--warmup_max_lr&#39;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;WarmupLR maximum LR value.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--warmup_num_steps&#39;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;WarmupLR step count for LR warmup.&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">parser</span>


<span class="k">def</span> <span class="nf">parse_arguments</span><span class="p">():</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">add_tuning_arguments</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span>

    <span class="n">lr_sched_args</span><span class="p">,</span> <span class="n">unknown_args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_known_args</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">lr_sched_args</span><span class="p">,</span> <span class="n">unknown_args</span>


<span class="k">def</span> <span class="nf">override_lr_range_test_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">LR_RANGE_TEST_MIN_LR</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_min_lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">LR_RANGE_TEST_MIN_LR</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_min_lr</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>
               <span class="n">LR_RANGE_TEST_STEP_RATE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_step_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">LR_RANGE_TEST_STEP_RATE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_step_rate</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>
               <span class="n">LR_RANGE_TEST_STEP_SIZE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_step_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">LR_RANGE_TEST_STEP_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_step_size</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>
               <span class="n">LR_RANGE_TEST_STAIRCASE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_staircase</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">LR_RANGE_TEST_STAIRCASE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_staircase</span>


<span class="k">def</span> <span class="nf">override_1cycle_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">CYCLE_FIRST_STEP_SIZE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_first_step_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_FIRST_STEP_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_first_step_size</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>
               <span class="n">CYCLE_FIRST_STAIR_COUNT</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_first_stair_count</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_FIRST_STAIR_COUNT</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_first_stair_count</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">CYCLE_SECOND_STEP_SIZE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_second_step_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_SECOND_STEP_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_second_step_size</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>
               <span class="n">CYCLE_SECOND_STAIR_COUNT</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_second_stair_count</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_SECOND_STAIR_COUNT</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_second_stair_count</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">DECAY_STEP_SIZE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">decay_step_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">DECAY_STEP_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">decay_step_size</span>

    <span class="c1"># 1Cycle LR params</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">CYCLE_MIN_LR</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_min_lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_MIN_LR</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_min_lr</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">CYCLE_MAX_LR</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_max_lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_MAX_LR</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_max_lr</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">DECAY_LR_RATE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">decay_lr_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">DECAY_LR_RATE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">decay_lr_rate</span>

    <span class="c1"># 1Cycle MOM params</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">CYCLE_MIN_MOM</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_min_mom</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_MIN_MOM</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_min_mom</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">CYCLE_MAX_MOM</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_max_mom</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_MAX_MOM</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_max_mom</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">DECAY_MOM_RATE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">decay_mom_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">DECAY_MOM_RATE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">decay_mom_rate</span>


<span class="k">def</span> <span class="nf">override_warmupLR_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">WARMUP_MIN_LR</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_min_lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">WARMUP_MIN_LR</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_min_lr</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">WARMUP_MAX_LR</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_max_lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">WARMUP_MAX_LR</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_max_lr</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">WARMUP_NUM_STEPS</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_num_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">WARMUP_NUM_STEPS</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_num_steps</span>


<span class="k">def</span> <span class="nf">override_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="c1"># LR range test params</span>
    <span class="n">override_lr_range_test_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># 1Cycle params</span>
    <span class="n">override_1cycle_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># WarmupLR params</span>
    <span class="n">override_warmupLR_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_config_from_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">LR_SCHEDULE</span><span class="p">)</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_schedule</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;--</span><span class="si">{}</span><span class="s1"> not specified on command line&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">LR_SCHEDULE</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_schedule</span> <span class="ow">in</span> <span class="n">VALID_LR_SCHEDULES</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not supported LR schedule&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">lr_schedule</span><span class="p">)</span>

    <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_schedule</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_schedule</span> <span class="o">==</span> <span class="n">LR_RANGE_TEST</span><span class="p">:</span>
        <span class="n">override_lr_range_test_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_schedule</span> <span class="o">==</span> <span class="n">ONE_CYCLE</span><span class="p">:</span>
        <span class="n">override_1cycle_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">override_warmupLR_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">config</span><span class="p">,</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">get_lr_from_config</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="s1">&#39;type&#39;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;LR schedule type not defined in config&#39;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="s1">&#39;params&#39;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;LR schedule params not defined in config&#39;</span>

    <span class="n">lr_schedule</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span>
    <span class="n">lr_params</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">lr_schedule</span> <span class="ow">in</span> <span class="n">VALID_LR_SCHEDULES</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not a valid LR schedule&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr_schedule</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">lr_schedule</span> <span class="o">==</span> <span class="n">LR_RANGE_TEST</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">lr_params</span><span class="p">[</span><span class="n">LR_RANGE_TEST_MIN_LR</span><span class="p">],</span> <span class="s1">&#39;&#39;</span>
    <span class="k">elif</span> <span class="n">lr_schedule</span> <span class="o">==</span> <span class="n">ONE_CYCLE</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">lr_params</span><span class="p">[</span><span class="n">CYCLE_MAX_LR</span><span class="p">],</span> <span class="s1">&#39;&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Warmup LR</span>
        <span class="k">return</span> <span class="n">lr_params</span><span class="p">[</span><span class="n">WARMUP_MAX_LR</span><span class="p">],</span> <span class="s1">&#39;&#39;</span>


<span class="k">class</span> <span class="nc">LRRangeTest</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the learning rate of each parameter group according to</span>
<span class="sd">    learning rate range test (LRRT) policy. The policy increases learning</span>
<span class="sd">    rate starting from a base value with a constant frequency, as detailed in</span>
<span class="sd">    the paper `A disciplined approach to neural network hyper-parameters: Part1`_.</span>

<span class="sd">    LRRT policy is used for finding maximum LR that trains a model without divergence, and can be used to</span>
<span class="sd">    configure the LR boundaries for Cylic LR schedules.</span>

<span class="sd">    LRRT changes the learning rate after every batch.</span>
<span class="sd">    `step` should be called after a batch has been used for training.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        lr_range_test_min_lr (float or list): Initial learning rate which is the</span>
<span class="sd">            lower boundary in the range test for each parameter group.</span>
<span class="sd">        lr_range_test_step_size (int): Interval of training steps to increase learning rate. Default: 2000</span>
<span class="sd">        lr_range_test_step_rate (float): Scaling rate for range test. Default: 1.0</span>
<span class="sd">        lr_range_test_staircase (bool): Scale in staircase fashion, rather than continous. Default: False.</span>
<span class="sd">        last_batch_iteration (int): The index of the last batch. This parameter is used when</span>
<span class="sd">            resuming a training job. Since `step()` should be invoked after each</span>
<span class="sd">            batch instead of after each epoch, this number represents the total</span>
<span class="sd">            number of *batches* computed, not the total number of epochs computed.</span>
<span class="sd">            When last_batch_iteration=-1, the schedule is started from the beginning.</span>
<span class="sd">            Default: -1</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span>
<span class="sd">        &gt;&gt;&gt; scheduler = torch.optim.LRRangeTest(optimizer)</span>
<span class="sd">        &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(10):</span>
<span class="sd">        &gt;&gt;&gt;     for batch in data_loader:</span>
<span class="sd">        &gt;&gt;&gt;         train_batch(...)</span>
<span class="sd">        &gt;&gt;&gt;         scheduler.step()</span>

<span class="sd">        _A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay:</span>
<span class="sd">        https://arxiv.org/abs/1803.09820</span>
<span class="sd">&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
                 <span class="n">lr_range_test_min_lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">lr_range_test_step_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
                 <span class="n">lr_range_test_step_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">lr_range_test_staircase</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">last_batch_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not an Optimizer&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">,</span>
                      <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">,</span>
                                          <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;expected </span><span class="si">{}</span><span class="s2"> lr_range_test_min_lr, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">),</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span> <span class="o">=</span> <span class="p">[</span><span class="n">lr_range_test_min_lr</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span> <span class="o">=</span> <span class="n">lr_range_test_step_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_rate</span> <span class="o">=</span> <span class="n">lr_range_test_step_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">last_batch_iteration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">staircase</span> <span class="o">=</span> <span class="n">lr_range_test_staircase</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">interval_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staircase_interval</span> <span class="k">if</span> <span class="n">lr_range_test_staircase</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_continous_interval</span>

        <span class="k">if</span> <span class="n">last_batch_iteration</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_update_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_staircase_interval</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_continous_interval</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span>

    <span class="k">def</span> <span class="nf">_get_increase</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">interval_fn</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">lr_increase</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_increase</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">lr_range_test_min_lr</span> <span class="o">*</span> <span class="n">lr_increase</span> <span class="k">for</span> <span class="n">lr_range_test_min_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">_update_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">group_lrs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">group_lrs</span><span class="p">):</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_iteration</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">batch_iteration</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">batch_iteration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sd</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">OneCycle</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the learning rate of each parameter group according to</span>
<span class="sd">    1Cycle learning rate policy (1CLR). 1CLR is a variation of the</span>
<span class="sd">    Cyclical Learning Rate (CLR) policy that involves one cycle followed by</span>
<span class="sd">    decay. The policy simultaneously cycles the learning rate (and momentum)</span>
<span class="sd">    between two boundaries with a constant frequency, as detailed in</span>
<span class="sd">    the paper `A disciplined approach to neural network hyper-parameters`_.</span>

<span class="sd">    1CLR policy changes the learning rate after every batch.</span>
<span class="sd">    `step` should be called after a batch has been used for training.</span>

<span class="sd">    This implementation was adapted from the github repo: `pytorch/pytorch`_</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        cycle_min_lr (float or list): Initial learning rate which is the</span>
<span class="sd">            lower boundary in the cycle for each parameter group.</span>
<span class="sd">        cycle_max_lr (float or list): Upper learning rate boundaries in the cycle</span>
<span class="sd">            for each parameter group. Functionally,</span>
<span class="sd">            it defines the cycle amplitude (cycle_max_lr - cycle_min_lr).</span>
<span class="sd">            The lr at any cycle is the sum of cycle_min_lr</span>
<span class="sd">            and some scaling of the amplitude; therefore</span>
<span class="sd">            cycle_max_lr may not actually be reached depending on</span>
<span class="sd">            scaling function.</span>
<span class="sd">        decay_lr_rate(float): Decay rate for learning rate. Default: 0.</span>
<span class="sd">        cycle_first_step_size (int): Number of training iterations in the</span>
<span class="sd">            increasing half of a cycle. Default: 2000</span>
<span class="sd">        cycle_second_step_size (int): Number of training iterations in the</span>
<span class="sd">            decreasing half of a cycle. If cycle_second_step_size is None,</span>
<span class="sd">            it is set to cycle_first_step_size. Default: None</span>
<span class="sd">        cycle_first_stair_count(int): Number of stairs in first half of cycle phase. This means</span>
<span class="sd">        lr/mom are changed in staircase fashion. Default 0, means staircase disabled.</span>
<span class="sd">        cycle_second_stair_count(int): Number of stairs in second half of cycle phase. This means</span>
<span class="sd">        lr/mom are changed in staircase fashion. Default 0, means staircase disabled.</span>
<span class="sd">        decay_step_size (int): Intervals for applying decay in decay phase. Default: 0, means no decay.</span>
<span class="sd">        cycle_momentum (bool): If ``True``, momentum is cycled inversely</span>
<span class="sd">            to learning rate between &#39;cycle_min_mom&#39; and &#39;cycle_max_mom&#39;.</span>
<span class="sd">            Default: True</span>
<span class="sd">        cycle_min_mom (float or list): Initial momentum which is the</span>
<span class="sd">            lower boundary in the cycle for each parameter group.</span>
<span class="sd">            Default: 0.8</span>
<span class="sd">        cycle_max_mom (float or list): Upper momentum boundaries in the cycle</span>
<span class="sd">            for each parameter group. Functionally,</span>
<span class="sd">            it defines the cycle amplitude (cycle_max_mom - cycle_min_mom).</span>
<span class="sd">            The momentum at any cycle is the difference of cycle_max_mom</span>
<span class="sd">            and some scaling of the amplitude; therefore</span>
<span class="sd">            cycle_min_mom may not actually be reached depending on</span>
<span class="sd">            scaling function. Default: 0.9</span>
<span class="sd">        decay_mom_rate (float): Decay rate for momentum. Default: 0.</span>
<span class="sd">        last_batch_iteration (int): The index of the last batch. This parameter is used when</span>
<span class="sd">            resuming a training job. Since `step()` should be invoked after each</span>
<span class="sd">            batch instead of after each epoch, this number represents the total</span>
<span class="sd">            number of *batches* computed, not the total number of epochs computed.</span>
<span class="sd">            When last_batch_iteration=-1, the schedule is started from the beginning.</span>
<span class="sd">            Default: -1</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span>
<span class="sd">        &gt;&gt;&gt; scheduler = torch.optim.OneCycle(optimizer)</span>
<span class="sd">        &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(10):</span>
<span class="sd">        &gt;&gt;&gt;     for batch in data_loader:</span>
<span class="sd">        &gt;&gt;&gt;         train_batch(...)</span>
<span class="sd">        &gt;&gt;&gt;         scheduler.step()</span>


<span class="sd">    .. _A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay: https://arxiv.org/abs/1803.09820</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">optimizer</span><span class="p">,</span>
                 <span class="n">cycle_min_lr</span><span class="p">,</span>
                 <span class="n">cycle_max_lr</span><span class="p">,</span>
                 <span class="n">decay_lr_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">cycle_first_step_size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                 <span class="n">cycle_second_step_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">cycle_first_stair_count</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">cycle_second_stair_count</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">decay_step_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">cycle_momentum</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">cycle_min_mom</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                 <span class="n">cycle_max_mom</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                 <span class="n">decay_mom_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">last_batch_iteration</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not an Optimizer&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">cycle_min_lr</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">last_batch_iteration</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">cycle_max_lr</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>

        <span class="n">cycle_first_step_size</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">cycle_first_step_size</span><span class="p">)</span>
        <span class="n">cycle_second_step_size</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span>
            <span class="n">cycle_second_step_size</span>
        <span class="p">)</span> <span class="k">if</span> <span class="n">cycle_second_step_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">cycle_first_step_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span> <span class="o">=</span> <span class="n">cycle_first_step_size</span> <span class="o">+</span> <span class="n">cycle_second_step_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span> <span class="o">=</span> <span class="n">cycle_first_step_size</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">first_stair_count</span> <span class="o">=</span> <span class="n">cycle_first_stair_count</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">second_stair_count</span> <span class="o">=</span> <span class="n">cycle_first_stair_count</span> <span class="k">if</span> <span class="n">cycle_second_stair_count</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">cycle_second_stair_count</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decay_lr_rate</span> <span class="o">=</span> <span class="n">decay_lr_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_mom_rate</span> <span class="o">=</span> <span class="n">decay_mom_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_step_size</span> <span class="o">=</span> <span class="n">decay_step_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">min_moms</span> <span class="o">=</span> <span class="p">[(</span><span class="n">cycle_min_mom</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_moms</span> <span class="o">=</span> <span class="p">[(</span><span class="n">cycle_max_mom</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span> <span class="o">=</span> <span class="n">cycle_momentum</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">last_batch_iteration</span>

        <span class="k">if</span> <span class="n">cycle_momentum</span><span class="p">:</span>
            <span class="k">if</span> <span class="s1">&#39;betas&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">defaults</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;optimizer must support betas with `cycle_momentum` option enabled&#39;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">last_batch_iteration</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_moms</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                    <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">momentum</span>

    <span class="k">def</span> <span class="nf">_get_cycle_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">cycle</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span> <span class="o">-</span> <span class="n">cycle</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span><span class="p">:</span>
            <span class="n">scale_factor</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scale_factor</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">cycle_min_lr</span><span class="p">,</span> <span class="n">cycle_max_lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span><span class="p">):</span>
            <span class="n">base_height</span> <span class="o">=</span> <span class="p">(</span><span class="n">cycle_max_lr</span> <span class="o">-</span> <span class="n">cycle_min_lr</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_factor</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">cycle_min_lr</span> <span class="o">+</span> <span class="n">base_height</span>
            <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span><span class="p">:</span>
            <span class="n">momentums</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">base_betas</span><span class="p">,</span> <span class="n">max_betas</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_moms</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_moms</span><span class="p">):</span>
                <span class="n">cycle_min_mom</span> <span class="o">=</span> <span class="n">base_betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">cycle_max_mom</span> <span class="o">=</span> <span class="n">max_betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">base_height</span> <span class="o">=</span> <span class="p">(</span><span class="n">cycle_max_mom</span> <span class="o">-</span> <span class="n">cycle_min_mom</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_factor</span>
                <span class="n">momentum</span> <span class="o">=</span> <span class="n">cycle_max_mom</span> <span class="o">-</span> <span class="n">base_height</span>
                <span class="n">momentums</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">momentum</span><span class="p">,</span> <span class="n">base_betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">momentum</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">momentums</span><span class="p">):</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">momentum</span>

        <span class="k">return</span> <span class="n">lrs</span>

    <span class="k">def</span> <span class="nf">_get_decay_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decay_batch_iteration</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates the learning rate at batch index. This function is used</span>
<span class="sd">        after the cycle completes and post cycle decaying of lr/mom is enabled.</span>
<span class="sd">        This function treats `self.last_batch_iteration` as the last batch index.</span>

<span class="sd">        If `self.cycle_momentum` is ``True``, this function has a side effect of</span>
<span class="sd">        updating the optimizer&#39;s momentum.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">decay_interval</span> <span class="o">=</span> <span class="n">decay_batch_iteration</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_step_size</span>

        <span class="n">lr_decay_factor</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_lr_rate</span> <span class="o">*</span> <span class="n">decay_interval</span><span class="p">)</span>
        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">cycle_min_lr</span> <span class="o">*</span> <span class="n">lr_decay_factor</span> <span class="k">for</span> <span class="n">cycle_min_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span><span class="p">:</span>
            <span class="n">mom_decay_factor</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_mom_rate</span> <span class="o">*</span> <span class="n">decay_interval</span><span class="p">)</span>
            <span class="n">momentums</span> <span class="o">=</span> <span class="p">[(</span><span class="n">beta0</span> <span class="o">*</span> <span class="n">mom_decay_factor</span><span class="p">,</span>
                          <span class="n">beta1</span><span class="p">)</span> <span class="k">for</span> <span class="n">beta0</span><span class="p">,</span>
                         <span class="n">beta1</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_moms</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">momentum</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">momentums</span><span class="p">):</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">momentum</span>

        <span class="k">return</span> <span class="n">lrs</span>

    <span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates the learning rate at batch index. This function treats</span>
<span class="sd">        `self.last_batch_iteration` as the last batch index.</span>

<span class="sd">        If `self.cycle_momentum` is ``True``, this function has a side effect of</span>
<span class="sd">        updating the optimizer&#39;s momentum.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_cycle_lr</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_decay_lr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_iteration</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">batch_iteration</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">batch_iteration</span>
        <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()):</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sd</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">WarmupLR</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Increase the learning rate of each parameter group from min lr to max lr</span>
<span class="sd">        over warmup_num_steps steps, and then fix at max lr.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">            warmup_min_lr (float or list): minimum learning rate. Default: 0</span>
<span class="sd">            warmup_max_lr (float or list): maximum learning rate. Default: 0.001</span>
<span class="sd">            warmup_num_steps (int): number of steps to warm up from min_lr to max_lr. Default: 1000</span>
<span class="sd">            last_batch_iteration (int): The index of the last batch. Default: -1.</span>
<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span>
<span class="sd">            &gt;&gt;&gt; scheduler = torch.optim.WarmupLR(optimizer)</span>
<span class="sd">            &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)</span>
<span class="sd">            &gt;&gt;&gt; for epoch in range(10):</span>
<span class="sd">            &gt;&gt;&gt;     for batch in data_loader:</span>
<span class="sd">            &gt;&gt;&gt;         train_batch(...)</span>
<span class="sd">            &gt;&gt;&gt;         scheduler.step()</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
                 <span class="n">warmup_min_lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">warmup_max_lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
                 <span class="n">warmup_num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">last_batch_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_param</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_min_lr</span><span class="p">,</span> <span class="s2">&quot;min_lr&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_param</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_max_lr</span><span class="p">,</span> <span class="s2">&quot;max_lr&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delta_lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">big</span> <span class="o">-</span> <span class="n">small</span> <span class="k">for</span> <span class="n">big</span><span class="p">,</span> <span class="n">small</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_num_steps</span> <span class="o">=</span> <span class="n">warmup_num_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inverse_log_warm_up</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">warmup_num_steps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">last_batch_iteration</span>

    <span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_gamma</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">min_lr</span> <span class="o">+</span> <span class="p">(</span><span class="n">delta_lr</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">)</span> <span class="k">for</span> <span class="n">min_lr</span><span class="p">,</span>
            <span class="n">delta_lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">delta_lrs</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">last_batch_iteration</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">last_batch_iteration</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">last_batch_iteration</span>
        <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()):</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sd</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_get_gamma</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_num_steps</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">inverse_log_warm_up</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">1.0</span>

    <span class="k">def</span> <span class="nf">_format_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">param_value</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param_value</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param_value</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_value</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;expected </span><span class="si">{}</span><span class="s2"> value for </span><span class="si">{}</span><span class="s2">, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">),</span>
                    <span class="n">param_name</span><span class="p">,</span>
                    <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="n">param_value</span><span class="p">)))</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">param_value</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">param_value</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
</pre></div>

  </div>

  </header>

  <section id="section-items">
    <h2 class="section-title" id="header-variables">Module variables</h2>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.ALLGATHER_SIZE" class="name">var <span class="ident">ALLGATHER_SIZE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.ALLGATHER_SIZE_DEFAULT" class="name">var <span class="ident">ALLGATHER_SIZE_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.CYCLE_FIRST_STAIR_COUNT" class="name">var <span class="ident">CYCLE_FIRST_STAIR_COUNT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.CYCLE_FIRST_STEP_SIZE" class="name">var <span class="ident">CYCLE_FIRST_STEP_SIZE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.CYCLE_MAX_LR" class="name">var <span class="ident">CYCLE_MAX_LR</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.CYCLE_MAX_MOM" class="name">var <span class="ident">CYCLE_MAX_MOM</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.CYCLE_MIN_LR" class="name">var <span class="ident">CYCLE_MIN_LR</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.CYCLE_MIN_MOM" class="name">var <span class="ident">CYCLE_MIN_MOM</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.CYCLE_SECOND_STAIR_COUNT" class="name">var <span class="ident">CYCLE_SECOND_STAIR_COUNT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.CYCLE_SECOND_STEP_SIZE" class="name">var <span class="ident">CYCLE_SECOND_STEP_SIZE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.DECAY_LR_RATE" class="name">var <span class="ident">DECAY_LR_RATE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.DECAY_MOM_RATE" class="name">var <span class="ident">DECAY_MOM_RATE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.DECAY_STEP_SIZE" class="name">var <span class="ident">DECAY_STEP_SIZE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.DISABLE_ALLGATHER" class="name">var <span class="ident">DISABLE_ALLGATHER</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.DISABLE_ALLGATHER_DEFAULT" class="name">var <span class="ident">DISABLE_ALLGATHER_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.DISABLE_ALLGATHER_FORMAT" class="name">var <span class="ident">DISABLE_ALLGATHER_FORMAT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.DUMP_STATE" class="name">var <span class="ident">DUMP_STATE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.DUMP_STATE_DEFAULT" class="name">var <span class="ident">DUMP_STATE_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.DUMP_STATE_FORMAT" class="name">var <span class="ident">DUMP_STATE_FORMAT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.EDGE_VALUE" class="name">var <span class="ident">EDGE_VALUE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16" class="name">var <span class="ident">FP16</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16_ENABLED" class="name">var <span class="ident">FP16_ENABLED</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16_ENABLED_DEFAULT" class="name">var <span class="ident">FP16_ENABLED_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16_FORMAT" class="name">var <span class="ident">FP16_FORMAT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16_HYSTERESIS" class="name">var <span class="ident">FP16_HYSTERESIS</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16_HYSTERESIS_DEFAULT" class="name">var <span class="ident">FP16_HYSTERESIS_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16_INITIAL_SCALE_POWER" class="name">var <span class="ident">FP16_INITIAL_SCALE_POWER</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16_INITIAL_SCALE_POWER_DEFAULT" class="name">var <span class="ident">FP16_INITIAL_SCALE_POWER_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16_LOSS_SCALE" class="name">var <span class="ident">FP16_LOSS_SCALE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16_LOSS_SCALE_DEFAULT" class="name">var <span class="ident">FP16_LOSS_SCALE_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16_LOSS_SCALE_WINDOW" class="name">var <span class="ident">FP16_LOSS_SCALE_WINDOW</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16_LOSS_SCALE_WINDOW_DEFAULT" class="name">var <span class="ident">FP16_LOSS_SCALE_WINDOW_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16_MIN_LOSS_SCALE" class="name">var <span class="ident">FP16_MIN_LOSS_SCALE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP16_MIN_LOSS_SCALE_DEFAULT" class="name">var <span class="ident">FP16_MIN_LOSS_SCALE_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP32_ALLREDUCE" class="name">var <span class="ident">FP32_ALLREDUCE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP32_ALLREDUCE_DEFAULT" class="name">var <span class="ident">FP32_ALLREDUCE_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.FP32_ALLREDUCE_FORMAT" class="name">var <span class="ident">FP32_ALLREDUCE_FORMAT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.GRADIENT_ACCUMULATION_FORMAT" class="name">var <span class="ident">GRADIENT_ACCUMULATION_FORMAT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.GRADIENT_ACCUMULATION_STEPS" class="name">var <span class="ident">GRADIENT_ACCUMULATION_STEPS</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.GRADIENT_ACCUMULATION_STEPS_DEFAULT" class="name">var <span class="ident">GRADIENT_ACCUMULATION_STEPS_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.GRADIENT_CLIPPING" class="name">var <span class="ident">GRADIENT_CLIPPING</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.GRADIENT_CLIPPING_DEFAULT" class="name">var <span class="ident">GRADIENT_CLIPPING_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.GRADIENT_CLIPPING_FORMAT" class="name">var <span class="ident">GRADIENT_CLIPPING_FORMAT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.LR_RANGE_TEST" class="name">var <span class="ident">LR_RANGE_TEST</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.LR_RANGE_TEST_MIN_LR" class="name">var <span class="ident">LR_RANGE_TEST_MIN_LR</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.LR_RANGE_TEST_STAIRCASE" class="name">var <span class="ident">LR_RANGE_TEST_STAIRCASE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.LR_RANGE_TEST_STEP_RATE" class="name">var <span class="ident">LR_RANGE_TEST_STEP_RATE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.LR_RANGE_TEST_STEP_SIZE" class="name">var <span class="ident">LR_RANGE_TEST_STEP_SIZE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.LR_SCHEDULE" class="name">var <span class="ident">LR_SCHEDULE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.MAX_GRAD_NORM" class="name">var <span class="ident">MAX_GRAD_NORM</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.MID_VALUE" class="name">var <span class="ident">MID_VALUE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.ONE_CYCLE" class="name">var <span class="ident">ONE_CYCLE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.OPTIMIZER" class="name">var <span class="ident">OPTIMIZER</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.OPTIMIZER_PARAMS" class="name">var <span class="ident">OPTIMIZER_PARAMS</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.OPTIMIZER_TYPE_DEFAULT" class="name">var <span class="ident">OPTIMIZER_TYPE_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.PRESCALE_GRADIENTS" class="name">var <span class="ident">PRESCALE_GRADIENTS</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.PRESCALE_GRADIENTS_DEFAULT" class="name">var <span class="ident">PRESCALE_GRADIENTS_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.PRESCALE_GRADIENTS_FORMAT" class="name">var <span class="ident">PRESCALE_GRADIENTS_FORMAT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.ROUTE_ENCODE" class="name">var <span class="ident">ROUTE_ENCODE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.ROUTE_EVAL" class="name">var <span class="ident">ROUTE_EVAL</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.ROUTE_PREDICT" class="name">var <span class="ident">ROUTE_PREDICT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.ROUTE_TRAIN" class="name">var <span class="ident">ROUTE_TRAIN</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.SCHEDULER" class="name">var <span class="ident">SCHEDULER</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.SCHEDULER_PARAMS" class="name">var <span class="ident">SCHEDULER_PARAMS</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.SCHEDULER_TYPE_DEFAULT" class="name">var <span class="ident">SCHEDULER_TYPE_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.SPARSE_GRADIENTS" class="name">var <span class="ident">SPARSE_GRADIENTS</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.SPARSE_GRADIENTS_DEFAULT" class="name">var <span class="ident">SPARSE_GRADIENTS_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.STEPS_PER_PRINT" class="name">var <span class="ident">STEPS_PER_PRINT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.STEPS_PER_PRINT_DEFAULT" class="name">var <span class="ident">STEPS_PER_PRINT_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD" class="name">var <span class="ident">TENSORBOARD</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_ENABLED" class="name">var <span class="ident">TENSORBOARD_ENABLED</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_ENABLED_DEFAULT" class="name">var <span class="ident">TENSORBOARD_ENABLED_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_FORMAT" class="name">var <span class="ident">TENSORBOARD_FORMAT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_JOB_NAME" class="name">var <span class="ident">TENSORBOARD_JOB_NAME</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_JOB_NAME_DEFAULT" class="name">var <span class="ident">TENSORBOARD_JOB_NAME_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_OUTPUT_PATH" class="name">var <span class="ident">TENSORBOARD_OUTPUT_PATH</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.TENSORBOARD_OUTPUT_PATH_DEFAULT" class="name">var <span class="ident">TENSORBOARD_OUTPUT_PATH_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.TRAIN_BATCH_SIZE" class="name">var <span class="ident">TRAIN_BATCH_SIZE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.TRAIN_BATCH_SIZE_DEFAULT" class="name">var <span class="ident">TRAIN_BATCH_SIZE_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.TRAIN_MICRO_BATCH_SIZE_PER_GPU" class="name">var <span class="ident">TRAIN_MICRO_BATCH_SIZE_PER_GPU</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.TRAIN_MICRO_BATCH_SIZE_PER_GPU_DEFAULT" class="name">var <span class="ident">TRAIN_MICRO_BATCH_SIZE_PER_GPU_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.TYPE" class="name">var <span class="ident">TYPE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.VALID_LR_SCHEDULES" class="name">var <span class="ident">VALID_LR_SCHEDULES</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.VOCABULARY_SIZE" class="name">var <span class="ident">VOCABULARY_SIZE</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.VOCABULARY_SIZE_DEFAULT" class="name">var <span class="ident">VOCABULARY_SIZE_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.VOCABULARY_SIZE_FORMAT" class="name">var <span class="ident">VOCABULARY_SIZE_FORMAT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.WALL_CLOCK_BREAKDOWN" class="name">var <span class="ident">WALL_CLOCK_BREAKDOWN</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.WALL_CLOCK_BREAKDOWN_DEFAULT" class="name">var <span class="ident">WALL_CLOCK_BREAKDOWN_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.WALL_CLOCK_BREAKDOWN_FORMAT" class="name">var <span class="ident">WALL_CLOCK_BREAKDOWN_FORMAT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.WARMUP_LR" class="name">var <span class="ident">WARMUP_LR</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.WARMUP_MAX_LR" class="name">var <span class="ident">WARMUP_MAX_LR</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.WARMUP_MIN_LR" class="name">var <span class="ident">WARMUP_MIN_LR</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.WARMUP_NUM_STEPS" class="name">var <span class="ident">WARMUP_NUM_STEPS</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.ZERO_FORMAT" class="name">var <span class="ident">ZERO_FORMAT</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.ZERO_OPTIMIZATION" class="name">var <span class="ident">ZERO_OPTIMIZATION</span></p>


  <div class="source_cont">
</div>

      </div>
      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.ZERO_OPTIMIZATION_DEFAULT" class="name">var <span class="ident">ZERO_OPTIMIZATION_DEFAULT</span></p>


  <div class="source_cont">
</div>

      </div>

    <h2 class="section-title" id="header-functions">Functions</h2>

  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.add_tuning_arguments">
    <p>def <span class="ident">add_tuning_arguments</span>(</p><p>parser)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.add_tuning_arguments', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.add_tuning_arguments" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">add_tuning_arguments</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span><span class="s1">&#39;Convergence Tuning&#39;</span><span class="p">,</span>
                                      <span class="s1">&#39;Convergence tuning configurations&#39;</span><span class="p">)</span>

    <span class="c1"># LR scheduler</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--lr_schedule&#39;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;LR schedule for training.&#39;</span><span class="p">)</span>

    <span class="c1"># Learning rate range test</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--lr_range_test_min_lr&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Starting lr value.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--lr_range_test_step_rate&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;scaling rate for LR range test.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--lr_range_test_step_size&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;training steps per LR change.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--lr_range_test_staircase&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;use staircase scaling for LR range test.&#39;</span><span class="p">)</span>

    <span class="c1"># OneCycle schedule</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_first_step_size&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;size of first step of 1Cycle schedule (training steps).&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_first_stair_count&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;first stair count for 1Cycle schedule.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--cycle_second_step_size&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;size of second step of 1Cycle schedule (default first_step_size).&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_second_stair_count&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;second stair count for 1Cycle schedule.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--decay_step_size&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;size of intervals for applying post cycle decay (training steps).&#39;</span><span class="p">)</span>

    <span class="c1"># 1Cycle LR</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_min_lr&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;1Cycle LR lower bound.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_max_lr&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;1Cycle LR upper bound.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--decay_lr_rate&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;post cycle LR decay rate.&#39;</span><span class="p">)</span>

    <span class="c1"># 1Cycle Momentum</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--cycle_momentum&#39;</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="n">action</span><span class="o">=</span><span class="s1">&#39;store_true&#39;</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Enable 1Cycle momentum schedule.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_min_mom&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;1Cycle momentum lower bound.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--cycle_max_mom&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;1Cycle momentum upper bound.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--decay_mom_rate&quot;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;post cycle momentum decay rate.&#39;</span><span class="p">)</span>

    <span class="c1"># Warmup LR</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--warmup_min_lr&#39;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;WarmupLR minimum/initial LR value&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--warmup_max_lr&#39;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;WarmupLR maximum LR value.&#39;</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--warmup_num_steps&#39;</span><span class="p">,</span>
                       <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                       <span class="n">default</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                       <span class="n">help</span><span class="o">=</span><span class="s1">&#39;WarmupLR step count for LR warmup.&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">parser</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.get_config_from_args">
    <p>def <span class="ident">get_config_from_args</span>(</p><p>args)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.get_config_from_args', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.get_config_from_args" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_config_from_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">LR_SCHEDULE</span><span class="p">)</span> <span class="ow">or</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_schedule</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;--</span><span class="si">{}</span><span class="s1"> not specified on command line&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">LR_SCHEDULE</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_schedule</span> <span class="ow">in</span> <span class="n">VALID_LR_SCHEDULES</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not supported LR schedule&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">lr_schedule</span><span class="p">)</span>

    <span class="n">config</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_schedule</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_schedule</span> <span class="o">==</span> <span class="n">LR_RANGE_TEST</span><span class="p">:</span>
        <span class="n">override_lr_range_test_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_schedule</span> <span class="o">==</span> <span class="n">ONE_CYCLE</span><span class="p">:</span>
        <span class="n">override_1cycle_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">override_warmupLR_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">config</span><span class="p">,</span> <span class="kc">None</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.get_lr_from_config">
    <p>def <span class="ident">get_lr_from_config</span>(</p><p>config)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.get_lr_from_config', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.get_lr_from_config" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_lr_from_config</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="s1">&#39;type&#39;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;LR schedule type not defined in config&#39;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="s1">&#39;params&#39;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;LR schedule params not defined in config&#39;</span>

    <span class="n">lr_schedule</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span>
    <span class="n">lr_params</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">lr_schedule</span> <span class="ow">in</span> <span class="n">VALID_LR_SCHEDULES</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not a valid LR schedule&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr_schedule</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">lr_schedule</span> <span class="o">==</span> <span class="n">LR_RANGE_TEST</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">lr_params</span><span class="p">[</span><span class="n">LR_RANGE_TEST_MIN_LR</span><span class="p">],</span> <span class="s1">&#39;&#39;</span>
    <span class="k">elif</span> <span class="n">lr_schedule</span> <span class="o">==</span> <span class="n">ONE_CYCLE</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">lr_params</span><span class="p">[</span><span class="n">CYCLE_MAX_LR</span><span class="p">],</span> <span class="s1">&#39;&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Warmup LR</span>
        <span class="k">return</span> <span class="n">lr_params</span><span class="p">[</span><span class="n">WARMUP_MAX_LR</span><span class="p">],</span> <span class="s1">&#39;&#39;</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.override_1cycle_params">
    <p>def <span class="ident">override_1cycle_params</span>(</p><p>args, params)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.override_1cycle_params', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.override_1cycle_params" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">override_1cycle_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">CYCLE_FIRST_STEP_SIZE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_first_step_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_FIRST_STEP_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_first_step_size</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>
               <span class="n">CYCLE_FIRST_STAIR_COUNT</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_first_stair_count</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_FIRST_STAIR_COUNT</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_first_stair_count</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">CYCLE_SECOND_STEP_SIZE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_second_step_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_SECOND_STEP_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_second_step_size</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>
               <span class="n">CYCLE_SECOND_STAIR_COUNT</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_second_stair_count</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_SECOND_STAIR_COUNT</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_second_stair_count</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">DECAY_STEP_SIZE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">decay_step_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">DECAY_STEP_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">decay_step_size</span>

    <span class="c1"># 1Cycle LR params</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">CYCLE_MIN_LR</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_min_lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_MIN_LR</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_min_lr</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">CYCLE_MAX_LR</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_max_lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_MAX_LR</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_max_lr</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">DECAY_LR_RATE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">decay_lr_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">DECAY_LR_RATE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">decay_lr_rate</span>

    <span class="c1"># 1Cycle MOM params</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">CYCLE_MIN_MOM</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_min_mom</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_MIN_MOM</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_min_mom</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">CYCLE_MAX_MOM</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_max_mom</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">CYCLE_MAX_MOM</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cycle_max_mom</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">DECAY_MOM_RATE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">decay_mom_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">DECAY_MOM_RATE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">decay_mom_rate</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.override_lr_range_test_params">
    <p>def <span class="ident">override_lr_range_test_params</span>(</p><p>args, params)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.override_lr_range_test_params', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.override_lr_range_test_params" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">override_lr_range_test_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">LR_RANGE_TEST_MIN_LR</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_min_lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">LR_RANGE_TEST_MIN_LR</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_min_lr</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>
               <span class="n">LR_RANGE_TEST_STEP_RATE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_step_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">LR_RANGE_TEST_STEP_RATE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_step_rate</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>
               <span class="n">LR_RANGE_TEST_STEP_SIZE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_step_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">LR_RANGE_TEST_STEP_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_step_size</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>
               <span class="n">LR_RANGE_TEST_STAIRCASE</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_staircase</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">LR_RANGE_TEST_STAIRCASE</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">lr_range_test_staircase</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.override_params">
    <p>def <span class="ident">override_params</span>(</p><p>args, params)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.override_params', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.override_params" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">override_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="c1"># LR range test params</span>
    <span class="n">override_lr_range_test_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># 1Cycle params</span>
    <span class="n">override_1cycle_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># WarmupLR params</span>
    <span class="n">override_warmupLR_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.override_warmupLR_params">
    <p>def <span class="ident">override_warmupLR_params</span>(</p><p>args, params)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.override_warmupLR_params', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.override_warmupLR_params" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">override_warmupLR_params</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">WARMUP_MIN_LR</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_min_lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">WARMUP_MIN_LR</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_min_lr</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">WARMUP_MAX_LR</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_max_lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">WARMUP_MAX_LR</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_max_lr</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">WARMUP_NUM_STEPS</span><span class="p">)</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_num_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="p">[</span><span class="n">WARMUP_NUM_STEPS</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">warmup_num_steps</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.parse_arguments">
    <p>def <span class="ident">parse_arguments</span>(</p><p>)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.parse_arguments', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.parse_arguments" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">parse_arguments</span><span class="p">():</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">add_tuning_arguments</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span>

    <span class="n">lr_sched_args</span><span class="p">,</span> <span class="n">unknown_args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_known_args</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">lr_sched_args</span><span class="p">,</span> <span class="n">unknown_args</span>
</pre></div>

  </div>
</div>

  </div>


    <h2 class="section-title" id="header-classes">Classes</h2>

      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest" class="name">class <span class="ident">LRRangeTest</span></p>


    <div class="desc"><p>Sets the learning rate of each parameter group according to
learning rate range test (LRRT) policy. The policy increases learning
rate starting from a base value with a constant frequency, as detailed in
the paper <code>A disciplined approach to neural network hyper-parameters: Part1</code>_.</p>
<p>LRRT policy is used for finding maximum LR that trains a model without divergence, and can be used to
configure the LR boundaries for Cylic LR schedules.</p>
<p>LRRT changes the learning rate after every batch.
<code>step</code> should be called after a batch has been used for training.</p>
<p>Args:
    optimizer (Optimizer): Wrapped optimizer.
    lr_range_test_min_lr (float or list): Initial learning rate which is the
        lower boundary in the range test for each parameter group.
    lr_range_test_step_size (int): Interval of training steps to increase learning rate. Default: 2000
    lr_range_test_step_rate (float): Scaling rate for range test. Default: 1.0
    lr_range_test_staircase (bool): Scale in staircase fashion, rather than continous. Default: False.
    last_batch_iteration (int): The index of the last batch. This parameter is used when
        resuming a training job. Since <code>step()</code> should be invoked after each
        batch instead of after each epoch, this number represents the total
        number of <em>batches</em> computed, not the total number of epochs computed.
        When last_batch_iteration=-1, the schedule is started from the beginning.
        Default: -1</p>
<p>Example:
    &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
    &gt;&gt;&gt; scheduler = torch.optim.LRRangeTest(optimizer)
    &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)
    &gt;&gt;&gt; for epoch in range(10):
    &gt;&gt;&gt;     for batch in data_loader:
    &gt;&gt;&gt;         train_batch(...)
    &gt;&gt;&gt;         scheduler.step()</p>
<div class="codehilite"><pre><span></span><span class="err">_A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay:</span>
<span class="c">https://arxiv.org/abs/1803.09820</span>
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.LRRangeTest', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.LRRangeTest" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">LRRangeTest</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the learning rate of each parameter group according to</span>
<span class="sd">    learning rate range test (LRRT) policy. The policy increases learning</span>
<span class="sd">    rate starting from a base value with a constant frequency, as detailed in</span>
<span class="sd">    the paper `A disciplined approach to neural network hyper-parameters: Part1`_.</span>

<span class="sd">    LRRT policy is used for finding maximum LR that trains a model without divergence, and can be used to</span>
<span class="sd">    configure the LR boundaries for Cylic LR schedules.</span>

<span class="sd">    LRRT changes the learning rate after every batch.</span>
<span class="sd">    `step` should be called after a batch has been used for training.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        lr_range_test_min_lr (float or list): Initial learning rate which is the</span>
<span class="sd">            lower boundary in the range test for each parameter group.</span>
<span class="sd">        lr_range_test_step_size (int): Interval of training steps to increase learning rate. Default: 2000</span>
<span class="sd">        lr_range_test_step_rate (float): Scaling rate for range test. Default: 1.0</span>
<span class="sd">        lr_range_test_staircase (bool): Scale in staircase fashion, rather than continous. Default: False.</span>
<span class="sd">        last_batch_iteration (int): The index of the last batch. This parameter is used when</span>
<span class="sd">            resuming a training job. Since `step()` should be invoked after each</span>
<span class="sd">            batch instead of after each epoch, this number represents the total</span>
<span class="sd">            number of *batches* computed, not the total number of epochs computed.</span>
<span class="sd">            When last_batch_iteration=-1, the schedule is started from the beginning.</span>
<span class="sd">            Default: -1</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span>
<span class="sd">        &gt;&gt;&gt; scheduler = torch.optim.LRRangeTest(optimizer)</span>
<span class="sd">        &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(10):</span>
<span class="sd">        &gt;&gt;&gt;     for batch in data_loader:</span>
<span class="sd">        &gt;&gt;&gt;         train_batch(...)</span>
<span class="sd">        &gt;&gt;&gt;         scheduler.step()</span>

<span class="sd">        _A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay:</span>
<span class="sd">        https://arxiv.org/abs/1803.09820</span>
<span class="sd">&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
                 <span class="n">lr_range_test_min_lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">lr_range_test_step_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
                 <span class="n">lr_range_test_step_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">lr_range_test_staircase</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">last_batch_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not an Optimizer&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">,</span>
                      <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">,</span>
                                          <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;expected </span><span class="si">{}</span><span class="s2"> lr_range_test_min_lr, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">),</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span> <span class="o">=</span> <span class="p">[</span><span class="n">lr_range_test_min_lr</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span> <span class="o">=</span> <span class="n">lr_range_test_step_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_rate</span> <span class="o">=</span> <span class="n">lr_range_test_step_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">last_batch_iteration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">staircase</span> <span class="o">=</span> <span class="n">lr_range_test_staircase</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">interval_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staircase_interval</span> <span class="k">if</span> <span class="n">lr_range_test_staircase</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_continous_interval</span>

        <span class="k">if</span> <span class="n">last_batch_iteration</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_update_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_staircase_interval</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_continous_interval</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span>

    <span class="k">def</span> <span class="nf">_get_increase</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">interval_fn</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">lr_increase</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_increase</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">lr_range_test_min_lr</span> <span class="o">*</span> <span class="n">lr_increase</span> <span class="k">for</span> <span class="n">lr_range_test_min_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">_update_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">group_lrs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">group_lrs</span><span class="p">):</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_iteration</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">batch_iteration</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">batch_iteration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sd</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">]</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#deepspeed.pt.deepspeed_lr_schedules.LRRangeTest">LRRangeTest</a></li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>

  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, optimizer, lr_range_test_min_lr=0.001, lr_range_test_step_size=2000, lr_range_test_step_rate=1.0, lr_range_test_staircase=False, last_batch_iteration=-1)</p>
    </div>




    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.__init__', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
             <span class="n">lr_range_test_min_lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span>
             <span class="n">lr_range_test_step_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
             <span class="n">lr_range_test_step_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
             <span class="n">lr_range_test_staircase</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
             <span class="n">last_batch_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not an Optimizer&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">,</span>
                  <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">,</span>
                                      <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;expected </span><span class="si">{}</span><span class="s2"> lr_range_test_min_lr, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">),</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">lr_range_test_min_lr</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span> <span class="o">=</span> <span class="p">[</span><span class="n">lr_range_test_min_lr</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span> <span class="o">=</span> <span class="n">lr_range_test_step_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">step_rate</span> <span class="o">=</span> <span class="n">lr_range_test_step_rate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">last_batch_iteration</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">staircase</span> <span class="o">=</span> <span class="n">lr_range_test_staircase</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">interval_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staircase_interval</span> <span class="k">if</span> <span class="n">lr_range_test_staircase</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_continous_interval</span>
    <span class="k">if</span> <span class="n">last_batch_iteration</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.get_lr">
    <p>def <span class="ident">get_lr</span>(</p><p>self)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.get_lr', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.get_lr" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">lr_increase</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_increase</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="n">lr_range_test_min_lr</span> <span class="o">*</span> <span class="n">lr_increase</span> <span class="k">for</span> <span class="n">lr_range_test_min_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span>
    <span class="p">]</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.load_state_dict">
    <p>def <span class="ident">load_state_dict</span>(</p><p>self, sd)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.load_state_dict', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.load_state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sd</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">]</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.state_dict">
    <p>def <span class="ident">state_dict</span>(</p><p>self)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.state_dict', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span><span class="p">}</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.step">
    <p>def <span class="ident">step</span>(</p><p>self, batch_iteration=None)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.step', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.step" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_iteration</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">batch_iteration</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">batch_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">batch_iteration</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_update_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">())</span>
</pre></div>

  </div>
</div>

  </div>

          <h3>Instance variables</h3>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.interval_fn" class="name">var <span class="ident">interval_fn</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.last_batch_iteration" class="name">var <span class="ident">last_batch_iteration</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.optimizer" class="name">var <span class="ident">optimizer</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.staircase" class="name">var <span class="ident">staircase</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.step_rate" class="name">var <span class="ident">step_rate</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.LRRangeTest.step_size" class="name">var <span class="ident">step_size</span></p>




  <div class="source_cont">
</div>

            </div>
      </div>
      </div>

      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle" class="name">class <span class="ident">OneCycle</span></p>


    <div class="desc"><p>Sets the learning rate of each parameter group according to
1Cycle learning rate policy (1CLR). 1CLR is a variation of the
Cyclical Learning Rate (CLR) policy that involves one cycle followed by
decay. The policy simultaneously cycles the learning rate (and momentum)
between two boundaries with a constant frequency, as detailed in
the paper <code>A disciplined approach to neural network hyper-parameters</code>_.</p>
<p>1CLR policy changes the learning rate after every batch.
<code>step</code> should be called after a batch has been used for training.</p>
<p>This implementation was adapted from the github repo: <code>pytorch/pytorch</code>_</p>
<p>Args:
    optimizer (Optimizer): Wrapped optimizer.
    cycle_min_lr (float or list): Initial learning rate which is the
        lower boundary in the cycle for each parameter group.
    cycle_max_lr (float or list): Upper learning rate boundaries in the cycle
        for each parameter group. Functionally,
        it defines the cycle amplitude (cycle_max_lr - cycle_min_lr).
        The lr at any cycle is the sum of cycle_min_lr
        and some scaling of the amplitude; therefore
        cycle_max_lr may not actually be reached depending on
        scaling function.
    decay_lr_rate(float): Decay rate for learning rate. Default: 0.
    cycle_first_step_size (int): Number of training iterations in the
        increasing half of a cycle. Default: 2000
    cycle_second_step_size (int): Number of training iterations in the
        decreasing half of a cycle. If cycle_second_step_size is None,
        it is set to cycle_first_step_size. Default: None
    cycle_first_stair_count(int): Number of stairs in first half of cycle phase. This means
    lr/mom are changed in staircase fashion. Default 0, means staircase disabled.
    cycle_second_stair_count(int): Number of stairs in second half of cycle phase. This means
    lr/mom are changed in staircase fashion. Default 0, means staircase disabled.
    decay_step_size (int): Intervals for applying decay in decay phase. Default: 0, means no decay.
    cycle_momentum (bool): If <code>True</code>, momentum is cycled inversely
        to learning rate between 'cycle_min_mom' and 'cycle_max_mom'.
        Default: True
    cycle_min_mom (float or list): Initial momentum which is the
        lower boundary in the cycle for each parameter group.
        Default: 0.8
    cycle_max_mom (float or list): Upper momentum boundaries in the cycle
        for each parameter group. Functionally,
        it defines the cycle amplitude (cycle_max_mom - cycle_min_mom).
        The momentum at any cycle is the difference of cycle_max_mom
        and some scaling of the amplitude; therefore
        cycle_min_mom may not actually be reached depending on
        scaling function. Default: 0.9
    decay_mom_rate (float): Decay rate for momentum. Default: 0.
    last_batch_iteration (int): The index of the last batch. This parameter is used when
        resuming a training job. Since <code>step()</code> should be invoked after each
        batch instead of after each epoch, this number represents the total
        number of <em>batches</em> computed, not the total number of epochs computed.
        When last_batch_iteration=-1, the schedule is started from the beginning.
        Default: -1</p>
<p>Example:
    &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
    &gt;&gt;&gt; scheduler = torch.optim.OneCycle(optimizer)
    &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)
    &gt;&gt;&gt; for epoch in range(10):
    &gt;&gt;&gt;     for batch in data_loader:
    &gt;&gt;&gt;         train_batch(...)
    &gt;&gt;&gt;         scheduler.step()</p>
<p>.. _A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay: https://arxiv.org/abs/1803.09820</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.OneCycle', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.OneCycle" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">OneCycle</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the learning rate of each parameter group according to</span>
<span class="sd">    1Cycle learning rate policy (1CLR). 1CLR is a variation of the</span>
<span class="sd">    Cyclical Learning Rate (CLR) policy that involves one cycle followed by</span>
<span class="sd">    decay. The policy simultaneously cycles the learning rate (and momentum)</span>
<span class="sd">    between two boundaries with a constant frequency, as detailed in</span>
<span class="sd">    the paper `A disciplined approach to neural network hyper-parameters`_.</span>

<span class="sd">    1CLR policy changes the learning rate after every batch.</span>
<span class="sd">    `step` should be called after a batch has been used for training.</span>

<span class="sd">    This implementation was adapted from the github repo: `pytorch/pytorch`_</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        cycle_min_lr (float or list): Initial learning rate which is the</span>
<span class="sd">            lower boundary in the cycle for each parameter group.</span>
<span class="sd">        cycle_max_lr (float or list): Upper learning rate boundaries in the cycle</span>
<span class="sd">            for each parameter group. Functionally,</span>
<span class="sd">            it defines the cycle amplitude (cycle_max_lr - cycle_min_lr).</span>
<span class="sd">            The lr at any cycle is the sum of cycle_min_lr</span>
<span class="sd">            and some scaling of the amplitude; therefore</span>
<span class="sd">            cycle_max_lr may not actually be reached depending on</span>
<span class="sd">            scaling function.</span>
<span class="sd">        decay_lr_rate(float): Decay rate for learning rate. Default: 0.</span>
<span class="sd">        cycle_first_step_size (int): Number of training iterations in the</span>
<span class="sd">            increasing half of a cycle. Default: 2000</span>
<span class="sd">        cycle_second_step_size (int): Number of training iterations in the</span>
<span class="sd">            decreasing half of a cycle. If cycle_second_step_size is None,</span>
<span class="sd">            it is set to cycle_first_step_size. Default: None</span>
<span class="sd">        cycle_first_stair_count(int): Number of stairs in first half of cycle phase. This means</span>
<span class="sd">        lr/mom are changed in staircase fashion. Default 0, means staircase disabled.</span>
<span class="sd">        cycle_second_stair_count(int): Number of stairs in second half of cycle phase. This means</span>
<span class="sd">        lr/mom are changed in staircase fashion. Default 0, means staircase disabled.</span>
<span class="sd">        decay_step_size (int): Intervals for applying decay in decay phase. Default: 0, means no decay.</span>
<span class="sd">        cycle_momentum (bool): If ``True``, momentum is cycled inversely</span>
<span class="sd">            to learning rate between &#39;cycle_min_mom&#39; and &#39;cycle_max_mom&#39;.</span>
<span class="sd">            Default: True</span>
<span class="sd">        cycle_min_mom (float or list): Initial momentum which is the</span>
<span class="sd">            lower boundary in the cycle for each parameter group.</span>
<span class="sd">            Default: 0.8</span>
<span class="sd">        cycle_max_mom (float or list): Upper momentum boundaries in the cycle</span>
<span class="sd">            for each parameter group. Functionally,</span>
<span class="sd">            it defines the cycle amplitude (cycle_max_mom - cycle_min_mom).</span>
<span class="sd">            The momentum at any cycle is the difference of cycle_max_mom</span>
<span class="sd">            and some scaling of the amplitude; therefore</span>
<span class="sd">            cycle_min_mom may not actually be reached depending on</span>
<span class="sd">            scaling function. Default: 0.9</span>
<span class="sd">        decay_mom_rate (float): Decay rate for momentum. Default: 0.</span>
<span class="sd">        last_batch_iteration (int): The index of the last batch. This parameter is used when</span>
<span class="sd">            resuming a training job. Since `step()` should be invoked after each</span>
<span class="sd">            batch instead of after each epoch, this number represents the total</span>
<span class="sd">            number of *batches* computed, not the total number of epochs computed.</span>
<span class="sd">            When last_batch_iteration=-1, the schedule is started from the beginning.</span>
<span class="sd">            Default: -1</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span>
<span class="sd">        &gt;&gt;&gt; scheduler = torch.optim.OneCycle(optimizer)</span>
<span class="sd">        &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(10):</span>
<span class="sd">        &gt;&gt;&gt;     for batch in data_loader:</span>
<span class="sd">        &gt;&gt;&gt;         train_batch(...)</span>
<span class="sd">        &gt;&gt;&gt;         scheduler.step()</span>


<span class="sd">    .. _A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay: https://arxiv.org/abs/1803.09820</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">optimizer</span><span class="p">,</span>
                 <span class="n">cycle_min_lr</span><span class="p">,</span>
                 <span class="n">cycle_max_lr</span><span class="p">,</span>
                 <span class="n">decay_lr_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">cycle_first_step_size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                 <span class="n">cycle_second_step_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">cycle_first_stair_count</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">cycle_second_stair_count</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">decay_step_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">cycle_momentum</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">cycle_min_mom</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                 <span class="n">cycle_max_mom</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                 <span class="n">decay_mom_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">last_batch_iteration</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not an Optimizer&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">cycle_min_lr</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">last_batch_iteration</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">cycle_max_lr</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>

        <span class="n">cycle_first_step_size</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">cycle_first_step_size</span><span class="p">)</span>
        <span class="n">cycle_second_step_size</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span>
            <span class="n">cycle_second_step_size</span>
        <span class="p">)</span> <span class="k">if</span> <span class="n">cycle_second_step_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">cycle_first_step_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span> <span class="o">=</span> <span class="n">cycle_first_step_size</span> <span class="o">+</span> <span class="n">cycle_second_step_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span> <span class="o">=</span> <span class="n">cycle_first_step_size</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">first_stair_count</span> <span class="o">=</span> <span class="n">cycle_first_stair_count</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">second_stair_count</span> <span class="o">=</span> <span class="n">cycle_first_stair_count</span> <span class="k">if</span> <span class="n">cycle_second_stair_count</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">cycle_second_stair_count</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decay_lr_rate</span> <span class="o">=</span> <span class="n">decay_lr_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_mom_rate</span> <span class="o">=</span> <span class="n">decay_mom_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_step_size</span> <span class="o">=</span> <span class="n">decay_step_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">min_moms</span> <span class="o">=</span> <span class="p">[(</span><span class="n">cycle_min_mom</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_moms</span> <span class="o">=</span> <span class="p">[(</span><span class="n">cycle_max_mom</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span> <span class="o">=</span> <span class="n">cycle_momentum</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">last_batch_iteration</span>

        <span class="k">if</span> <span class="n">cycle_momentum</span><span class="p">:</span>
            <span class="k">if</span> <span class="s1">&#39;betas&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">defaults</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;optimizer must support betas with `cycle_momentum` option enabled&#39;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">last_batch_iteration</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_moms</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                    <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">momentum</span>

    <span class="k">def</span> <span class="nf">_get_cycle_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">cycle</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span> <span class="o">-</span> <span class="n">cycle</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span><span class="p">:</span>
            <span class="n">scale_factor</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scale_factor</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">cycle_min_lr</span><span class="p">,</span> <span class="n">cycle_max_lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span><span class="p">):</span>
            <span class="n">base_height</span> <span class="o">=</span> <span class="p">(</span><span class="n">cycle_max_lr</span> <span class="o">-</span> <span class="n">cycle_min_lr</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_factor</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">cycle_min_lr</span> <span class="o">+</span> <span class="n">base_height</span>
            <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span><span class="p">:</span>
            <span class="n">momentums</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">base_betas</span><span class="p">,</span> <span class="n">max_betas</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_moms</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_moms</span><span class="p">):</span>
                <span class="n">cycle_min_mom</span> <span class="o">=</span> <span class="n">base_betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">cycle_max_mom</span> <span class="o">=</span> <span class="n">max_betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">base_height</span> <span class="o">=</span> <span class="p">(</span><span class="n">cycle_max_mom</span> <span class="o">-</span> <span class="n">cycle_min_mom</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_factor</span>
                <span class="n">momentum</span> <span class="o">=</span> <span class="n">cycle_max_mom</span> <span class="o">-</span> <span class="n">base_height</span>
                <span class="n">momentums</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">momentum</span><span class="p">,</span> <span class="n">base_betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">momentum</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">momentums</span><span class="p">):</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">momentum</span>

        <span class="k">return</span> <span class="n">lrs</span>

    <span class="k">def</span> <span class="nf">_get_decay_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decay_batch_iteration</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates the learning rate at batch index. This function is used</span>
<span class="sd">        after the cycle completes and post cycle decaying of lr/mom is enabled.</span>
<span class="sd">        This function treats `self.last_batch_iteration` as the last batch index.</span>

<span class="sd">        If `self.cycle_momentum` is ``True``, this function has a side effect of</span>
<span class="sd">        updating the optimizer&#39;s momentum.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">decay_interval</span> <span class="o">=</span> <span class="n">decay_batch_iteration</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_step_size</span>

        <span class="n">lr_decay_factor</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_lr_rate</span> <span class="o">*</span> <span class="n">decay_interval</span><span class="p">)</span>
        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">cycle_min_lr</span> <span class="o">*</span> <span class="n">lr_decay_factor</span> <span class="k">for</span> <span class="n">cycle_min_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span><span class="p">:</span>
            <span class="n">mom_decay_factor</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_mom_rate</span> <span class="o">*</span> <span class="n">decay_interval</span><span class="p">)</span>
            <span class="n">momentums</span> <span class="o">=</span> <span class="p">[(</span><span class="n">beta0</span> <span class="o">*</span> <span class="n">mom_decay_factor</span><span class="p">,</span>
                          <span class="n">beta1</span><span class="p">)</span> <span class="k">for</span> <span class="n">beta0</span><span class="p">,</span>
                         <span class="n">beta1</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_moms</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">momentum</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">momentums</span><span class="p">):</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">momentum</span>

        <span class="k">return</span> <span class="n">lrs</span>

    <span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates the learning rate at batch index. This function treats</span>
<span class="sd">        `self.last_batch_iteration` as the last batch index.</span>

<span class="sd">        If `self.cycle_momentum` is ``True``, this function has a side effect of</span>
<span class="sd">        updating the optimizer&#39;s momentum.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_cycle_lr</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_decay_lr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_iteration</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">batch_iteration</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">batch_iteration</span>
        <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()):</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sd</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">]</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#deepspeed.pt.deepspeed_lr_schedules.OneCycle">OneCycle</a></li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>

  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, optimizer, cycle_min_lr, cycle_max_lr, decay_lr_rate=0.0, cycle_first_step_size=2000, cycle_second_step_size=None, cycle_first_stair_count=0, cycle_second_stair_count=None, decay_step_size=0, cycle_momentum=True, cycle_min_mom=0.8, cycle_max_mom=0.9, decay_mom_rate=0.0, last_batch_iteration=-1)</p>
    </div>




    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.OneCycle.__init__', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.OneCycle.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">optimizer</span><span class="p">,</span>
             <span class="n">cycle_min_lr</span><span class="p">,</span>
             <span class="n">cycle_max_lr</span><span class="p">,</span>
             <span class="n">decay_lr_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
             <span class="n">cycle_first_step_size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
             <span class="n">cycle_second_step_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">cycle_first_stair_count</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
             <span class="n">cycle_second_stair_count</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">decay_step_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
             <span class="n">cycle_momentum</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">cycle_min_mom</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
             <span class="n">cycle_max_mom</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
             <span class="n">decay_mom_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
             <span class="n">last_batch_iteration</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> is not an Optimizer&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">cycle_min_lr</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">last_batch_iteration</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
            <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">cycle_max_lr</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
    <span class="n">cycle_first_step_size</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">cycle_first_step_size</span><span class="p">)</span>
    <span class="n">cycle_second_step_size</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span>
        <span class="n">cycle_second_step_size</span>
    <span class="p">)</span> <span class="k">if</span> <span class="n">cycle_second_step_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">cycle_first_step_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span> <span class="o">=</span> <span class="n">cycle_first_step_size</span> <span class="o">+</span> <span class="n">cycle_second_step_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span> <span class="o">=</span> <span class="n">cycle_first_step_size</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">first_stair_count</span> <span class="o">=</span> <span class="n">cycle_first_stair_count</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">second_stair_count</span> <span class="o">=</span> <span class="n">cycle_first_stair_count</span> <span class="k">if</span> <span class="n">cycle_second_stair_count</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">cycle_second_stair_count</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decay_lr_rate</span> <span class="o">=</span> <span class="n">decay_lr_rate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decay_mom_rate</span> <span class="o">=</span> <span class="n">decay_mom_rate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decay_step_size</span> <span class="o">=</span> <span class="n">decay_step_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_moms</span> <span class="o">=</span> <span class="p">[(</span><span class="n">cycle_min_mom</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_moms</span> <span class="o">=</span> <span class="p">[(</span><span class="n">cycle_max_mom</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span> <span class="o">=</span> <span class="n">cycle_momentum</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">last_batch_iteration</span>
    <span class="k">if</span> <span class="n">cycle_momentum</span><span class="p">:</span>
        <span class="k">if</span> <span class="s1">&#39;betas&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">defaults</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;optimizer must support betas with `cycle_momentum` option enabled&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">last_batch_iteration</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_moms</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">momentum</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.get_lr">
    <p>def <span class="ident">get_lr</span>(</p><p>self)</p>
    </div>




    <div class="desc"><p>Calculates the learning rate at batch index. This function treats
<code>self.last_batch_iteration</code> as the last batch index.</p>
<p>If <code>self.cycle_momentum</code> is <code>True</code>, this function has a side effect of
updating the optimizer's momentum.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.OneCycle.get_lr', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.OneCycle.get_lr" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculates the learning rate at batch index. This function treats</span>
<span class="sd">    `self.last_batch_iteration` as the last batch index.</span>
<span class="sd">    If `self.cycle_momentum` is ``True``, this function has a side effect of</span>
<span class="sd">    updating the optimizer&#39;s momentum.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_cycle_lr</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_decay_lr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.load_state_dict">
    <p>def <span class="ident">load_state_dict</span>(</p><p>self, sd)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.OneCycle.load_state_dict', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.OneCycle.load_state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sd</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">]</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.state_dict">
    <p>def <span class="ident">state_dict</span>(</p><p>self)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.OneCycle.state_dict', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.OneCycle.state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span><span class="p">}</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.step">
    <p>def <span class="ident">step</span>(</p><p>self, batch_iteration=None)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.OneCycle.step', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.OneCycle.step" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_iteration</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">batch_iteration</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">batch_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">batch_iteration</span>
    <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()):</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
</pre></div>

  </div>
</div>

  </div>

          <h3>Instance variables</h3>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.cycle_momentum" class="name">var <span class="ident">cycle_momentum</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.decay_lr_rate" class="name">var <span class="ident">decay_lr_rate</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.decay_mom_rate" class="name">var <span class="ident">decay_mom_rate</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.decay_step_size" class="name">var <span class="ident">decay_step_size</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.first_stair_count" class="name">var <span class="ident">first_stair_count</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.last_batch_iteration" class="name">var <span class="ident">last_batch_iteration</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.max_lrs" class="name">var <span class="ident">max_lrs</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.max_moms" class="name">var <span class="ident">max_moms</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.min_lrs" class="name">var <span class="ident">min_lrs</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.min_moms" class="name">var <span class="ident">min_moms</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.optimizer" class="name">var <span class="ident">optimizer</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.second_stair_count" class="name">var <span class="ident">second_stair_count</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.step_ratio" class="name">var <span class="ident">step_ratio</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.OneCycle.total_size" class="name">var <span class="ident">total_size</span></p>




  <div class="source_cont">
</div>

            </div>
      </div>
      </div>

      <div class="item">
      <p id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR" class="name">class <span class="ident">WarmupLR</span></p>


    <div class="desc"><p>Increase the learning rate of each parameter group from min lr to max lr
over warmup_num_steps steps, and then fix at max lr.</p>
<p>Args:
    optimizer (Optimizer): Wrapped optimizer.
    warmup_min_lr (float or list): minimum learning rate. Default: 0
    warmup_max_lr (float or list): maximum learning rate. Default: 0.001
    warmup_num_steps (int): number of steps to warm up from min_lr to max_lr. Default: 1000
    last_batch_iteration (int): The index of the last batch. Default: -1.
Example:
    &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
    &gt;&gt;&gt; scheduler = torch.optim.WarmupLR(optimizer)
    &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)
    &gt;&gt;&gt; for epoch in range(10):
    &gt;&gt;&gt;     for batch in data_loader:
    &gt;&gt;&gt;         train_batch(...)
    &gt;&gt;&gt;         scheduler.step()</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.WarmupLR', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.WarmupLR" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">WarmupLR</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Increase the learning rate of each parameter group from min lr to max lr</span>
<span class="sd">        over warmup_num_steps steps, and then fix at max lr.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">            warmup_min_lr (float or list): minimum learning rate. Default: 0</span>
<span class="sd">            warmup_max_lr (float or list): maximum learning rate. Default: 0.001</span>
<span class="sd">            warmup_num_steps (int): number of steps to warm up from min_lr to max_lr. Default: 1000</span>
<span class="sd">            last_batch_iteration (int): The index of the last batch. Default: -1.</span>
<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span>
<span class="sd">            &gt;&gt;&gt; scheduler = torch.optim.WarmupLR(optimizer)</span>
<span class="sd">            &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)</span>
<span class="sd">            &gt;&gt;&gt; for epoch in range(10):</span>
<span class="sd">            &gt;&gt;&gt;     for batch in data_loader:</span>
<span class="sd">            &gt;&gt;&gt;         train_batch(...)</span>
<span class="sd">            &gt;&gt;&gt;         scheduler.step()</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
                 <span class="n">warmup_min_lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">warmup_max_lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
                 <span class="n">warmup_num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">last_batch_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_param</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_min_lr</span><span class="p">,</span> <span class="s2">&quot;min_lr&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_param</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_max_lr</span><span class="p">,</span> <span class="s2">&quot;max_lr&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delta_lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">big</span> <span class="o">-</span> <span class="n">small</span> <span class="k">for</span> <span class="n">big</span><span class="p">,</span> <span class="n">small</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_num_steps</span> <span class="o">=</span> <span class="n">warmup_num_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inverse_log_warm_up</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">warmup_num_steps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">last_batch_iteration</span>

    <span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_gamma</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">min_lr</span> <span class="o">+</span> <span class="p">(</span><span class="n">delta_lr</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">)</span> <span class="k">for</span> <span class="n">min_lr</span><span class="p">,</span>
            <span class="n">delta_lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">delta_lrs</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">last_batch_iteration</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">last_batch_iteration</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">last_batch_iteration</span>
        <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()):</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sd</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_get_gamma</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_num_steps</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">inverse_log_warm_up</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">1.0</span>

    <span class="k">def</span> <span class="nf">_format_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">param_value</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param_value</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param_value</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_value</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;expected </span><span class="si">{}</span><span class="s2"> value for </span><span class="si">{}</span><span class="s2">, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">),</span>
                    <span class="n">param_name</span><span class="p">,</span>
                    <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="n">param_value</span><span class="p">)))</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">param_value</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">param_value</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#deepspeed.pt.deepspeed_lr_schedules.WarmupLR">WarmupLR</a></li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>

  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, optimizer, warmup_min_lr=0.0, warmup_max_lr=0.001, warmup_num_steps=1000, last_batch_iteration=-1)</p>
    </div>




    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.WarmupLR.__init__', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.WarmupLR.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
             <span class="n">warmup_min_lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
             <span class="n">warmup_max_lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
             <span class="n">warmup_num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
             <span class="n">last_batch_iteration</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_param</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_min_lr</span><span class="p">,</span> <span class="s2">&quot;min_lr&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_param</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_max_lr</span><span class="p">,</span> <span class="s2">&quot;max_lr&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">delta_lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">big</span> <span class="o">-</span> <span class="n">small</span> <span class="k">for</span> <span class="n">big</span><span class="p">,</span> <span class="n">small</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">warmup_num_steps</span> <span class="o">=</span> <span class="n">warmup_num_steps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inverse_log_warm_up</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">warmup_num_steps</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">last_batch_iteration</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.get_lr">
    <p>def <span class="ident">get_lr</span>(</p><p>self)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.WarmupLR.get_lr', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.WarmupLR.get_lr" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_gamma</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="n">min_lr</span> <span class="o">+</span> <span class="p">(</span><span class="n">delta_lr</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">)</span> <span class="k">for</span> <span class="n">min_lr</span><span class="p">,</span>
        <span class="n">delta_lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">delta_lrs</span><span class="p">)</span>
    <span class="p">]</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.load_state_dict">
    <p>def <span class="ident">load_state_dict</span>(</p><p>self, sd)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.WarmupLR.load_state_dict', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.WarmupLR.load_state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sd</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">sd</span><span class="p">[</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">]</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.state_dict">
    <p>def <span class="ident">state_dict</span>(</p><p>self)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.WarmupLR.state_dict', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.WarmupLR.state_dict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;last_batch_iteration&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span><span class="p">}</span>
</pre></div>

  </div>
</div>

  </div>


  <div class="item">
    <div class="name def" id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.step">
    <p>def <span class="ident">step</span>(</p><p>self, last_batch_iteration=None)</p>
    </div>




  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-deepspeed.pt.deepspeed_lr_schedules.WarmupLR.step', this);">Show source &equiv;</a></p>
  <div id="source-deepspeed.pt.deepspeed_lr_schedules.WarmupLR.step" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">last_batch_iteration</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">last_batch_iteration</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_batch_iteration</span> <span class="o">=</span> <span class="n">last_batch_iteration</span>
    <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()):</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
</pre></div>

  </div>
</div>

  </div>

          <h3>Instance variables</h3>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.delta_lrs" class="name">var <span class="ident">delta_lrs</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.inverse_log_warm_up" class="name">var <span class="ident">inverse_log_warm_up</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.last_batch_iteration" class="name">var <span class="ident">last_batch_iteration</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.max_lrs" class="name">var <span class="ident">max_lrs</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.min_lrs" class="name">var <span class="ident">min_lrs</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.optimizer" class="name">var <span class="ident">optimizer</span></p>




  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="deepspeed.pt.deepspeed_lr_schedules.WarmupLR.warmup_num_steps" class="name">var <span class="ident">warmup_num_steps</span></p>




  <div class="source_cont">
</div>

            </div>
      </div>
      </div>

  </section>

    </article>
  <div class="clear"> </div>
  <footer id="footer">
    <p>
      Documentation generated by
      <a href="https://github.com/BurntSushi/pdoc">pdoc 0.3.2</a>
    </p>

    <p>pdoc is in the public domain with the
      <a href="http://unlicense.org">UNLICENSE</a></p>

    <p>Design by <a href="http://nadh.in">Kailash Nadh</a></p>
  </footer>
</div>
</body>
</html>
