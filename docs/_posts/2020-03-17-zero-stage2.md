---
title: "ZeRO stage 2"
sneak_preview: true
excerpt: "Reduce memory footprint to enable training 10B models without model parallelism!"
---
* Reduce memory footprint of gradients
* Train larger models: e.g., 10B parameters on 32GPUs without model parallelism
* Train larger batch sizes

## Further updates coming soon!
