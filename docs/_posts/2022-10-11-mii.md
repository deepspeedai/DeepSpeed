---
title: "DeepSpeed Model Implementations for Inference (MII)"
excerpt: ""
date: 2022-10-11 00:09:00
tags: inference
---

![Text Generation Models](http://rasley.io/DeepSpeed/assets/images/mii/hero.png){: .align-center}

The Deep Learning (DL) open-source community has seen tremendous growth in the last few months. Incredibly powerful text generation models such as the Bloom 176B, or image generation model such as Stable Diffusion are now available to anyone with access to a handful or even a single GPU through platforms such as Hugging Face. While open sourcing has democratized access to AI capabilities, their application is still restricted by two critical factors: inference latency and cost.

There has been significant progress in system optimizations for DL model inference that can drastically reduce both latency and cost, but those are not easily accessible. A main reason for this limited accessibility is that the DL model inference landscape is diverse with models varying in size, architecture, system performance characteristics, hardware requirements, etc. Identifying the appropriate set of system optimizations applicable to a given model and applying them correctly is often beyond the scope of most data scientists, making low latency and low-cost inference mostly inaccessible.

DeepSpeed-MII is a new open-source python library from DeepSpeed, aimed towards making low-latency, low-cost inference of powerful models not only feasible but also easily accessible.

* MII offers access to highly optimized implementation of **thousands of widely used DL models.**
* MII supported models achieve significantly lower latency and cost compared to their original implementation. For example, **MII reduces the latency of Big-Science Bloom 176B model by 5.7x, while reducing the cost by over 40x** (*Figures B and J*). Similarly, **it reduces the latency and cost of deploying Stable Diffusion by 1.9x.** (*Figure C*)
* To enable low latency/cost inference, MII leverages an extensive set of optimizations from DeepSpeed-Inference such as *deepfusion* for transformers, automated *tensor-slicing* for multi-GPU inference, on-the-fly quantization with *ZeroQuant*, and several others (see below for more details).
* With state-of-the-art performance, MII supports low-cost deployment of these models both on-premises and on Azure via AML with just a **few lines of codes**.

# How does MII work?

![Text Generation Models](http://rasley.io/DeepSpeed/assets/images/mii/mii-arch.png)

*Figure A: MII Architecture, showing how MII automatically optimizes OSS models using DS-Inference before deploying them on-premises using GRPC, or on Microsoft Azure using AML Inference.*


Under-the-hood MII is powered by [DeepSpeed-Inference](https://arxiv.org/abs/2207.00032). Based on model type, model size, batch size, and available hardware resources, MII automatically applies the appropriate set of system optimizations from DeepSpeed-Inference to minimize latency and maximize throughput. It does so by using one of many pre-specified model injection policies, that allows MII and DeepSpeed-Inference to identify the underlying PyTorch model architecture and replace it with an optimized implementation (see *Figure A*). In doing so, MII makes the expansive set of optimizations in DeepSpeed-Inference automatically available for thousands of popular models that it supports.

# Supported Models and Tasks

MII supports a growing list of tasks such as text-generation, question-answering, text-classification, etc, across thousands of transformer models available through multiple open-sourced model repositories such as Hugging Face, FairSeq, EluetherAI, etc. It supports dense models based on BERT, RoBERTa, GPT, OPT, and BLOOM architectures ranging from few hundred million parameters in size to hundreds of billions of parameters in size. At the same time, it supports recent image generation models such as Stable Diffusion.

See the MII GitHub repo for an up-to-date list of [models and tasks supported by MII](https://github.com/microsoft/deepspeed-mii#supported-models-and-tasks).

# Inference Optimizations with MII

Here we provide a summary of the expansive set of optimizations from DeepSpeed-inference made available via MII. For more details, please refer to \[[1](https://arxiv.org/abs/2207.00032), [2](https://arxiv.org/abs/2206.01861)\]:

**DeepFusion for Transformers:** For transformer-based models such as Bert, Roberta, GPT-2, and GPT-J, MII leverages the transformer kernels in DeepSpeed-Inference that are optimized to achieve low latency at small batch size and high throughput at large batch sizes using DeepFusion.

**Multi-GPU Inference with Tensor-Slicing:** For massive models such as Bloom 176B, MII automatically enables tensor-parallelism within a node to leverage aggregate memory bandwidth and compute across multiple-GPUs to achieve lowest latency and throughput compared to anything else that is currently available.

**INT8 Inference with ZeroQuant:** For massive models with tens or hundreds of billions of parameters, MII supports INT8 Inference with ZeroQuant. Using this feature not only reduces the memory footprint and the number of GPUs required for inference, but also increases the inference throughput by supporting larger batch sizes and using INT8 compute, thus lowering cost compared to FP16.

**ZeRO-Inference for Resource Constrained Systems:** Models such as Bloom 176B, require over 176 GB of memory to just fit the model even with INT8 support. In the absence of the aggregate GPU memory across multiple GPUs required to inference such models, MII enables [ZeRO-Inference](https://www.deepspeed.ai/2022/09/09/zero-inference.html) that can leverage the system CPU memory to inference these massive models with a single GPU with limited memory.

**Compiler Optimizations:** When applicable, MII automatically applies compiler-based optimizations via [TorchScript](https://pytorch.org/docs/stable/jit.html), [nvFuser](https://pytorch.org/blog/introducing-nvfuser-a-deep-learning-compiler-for-pytorch/), and [CUDA graph](https://developer.nvidia.com/blog/cuda-graphs/), in addition to the above optimizations, to further lower latency and improve throughput.

# Quantifying Latency and Cost Reduction

Inference workloads can be either latency critical, where the primary objective is to minimize latency, or cost sensitive, where the primary objective is to minimize cost. In this section, we quantify the benefits of using MII for both latency critical and cost sensitive scenarios.

MII can work with two variations of DeepSpeed-Inference with different latency and cost implications. The first, referred to as ds-generic, contains most of the optimizations discussed above, and is also available via our open-source DeepSpeed library. The second, referred to as ds-azure, offers a tighter integration with Azure, and is available via MII to Microsoft Azure customers.  Here, we quantify the latency and cost reduction for both variations.

## Latency Critical Scenarios

For latency critical scenarios, where a small batch size of 1 is often used, MII can reduce the latency by up to 6x for a wide range of open-source models, across multiple tasks. More specifically, we show latency reduction of:

1. Up to 5.7x for multi-GPU inference for text generation using massive models such as Big Science Bloom, Facebook OPT and EluetherAI NeoX (*Figure B*)

2. Up to 1.9x for image generation tasks model using Stable Diffusion (*Figure C*)

3. Relatively smaller text generation models (up to 7B parameters) based on BLOOM, GPT, and OPT architectures, running on a single GPU (*Figures D, E, and F*)

4. Various text representation tasks like fill-mask, text-classification, question-answering, and token-classification using RoBERTa and BERT based models (*Figures G and H*).

![multi gpu latency](http://rasley.io/DeepSpeed/assets/images/mii/multi-gpu-latency.png){: .align-center}
*Figure B: Best achievable latency for large models. MII-Azure (int8) offers 5.7X lower latency compared to Baseline for Bloom-176B*

![stable diffusion](http://rasley.io/DeepSpeed/assets/images/mii/sd-latency.png){: .align-center}
*Figure C: Stable Diffusion text to image generation latency comparison*

![Bloom Models](http://rasley.io/DeepSpeed/assets/images/mii/bloom.png){: .align-center}
*Figure D: Latency comparison for BLOOM models. MII with DeepSpeed-Azure is up to 2.8x faster than baseline.*

![GPT Models](http://rasley.io/DeepSpeed/assets/images/mii/gpt.png){: .align-center}
*Figure E: Latency comparison for GPT models. MII with DeepSpeed-Azure is up to 3x faster than baseline.*

![OPT Models](http://rasley.io/DeepSpeed/assets/images/mii/opt.png){: .align-center}
*Figure F: Latency comparison for OPT models. MII with DeepSpeed-Azure is up to 2.8x faster than baseline.*

![Roberta Models](http://rasley.io/DeepSpeed/assets/images/mii/roberta.png){: .align-center}
*Figure G: Latency comparison for RoBERTa models. MII offers up to 9x lower model latency and up to 3x lower end-to-end latency than baseline on several tasks and RoBERTa variants [^overhead_details].*

![Bert Models](http://rasley.io/DeepSpeed/assets/images/mii/bert.png){: .align-center}
*Figure H: Latency comparison for BERT models. MII offers up to 8.9x lower model-latency and up to 4.5x end-to-end latency across several tasks and BERT variants[^overhead_details].*

[^overhead_details]: The end-to-end latency of an inference workload is comprised of two components: i) actual model execution, and ii) pre/post processing before and after the model execution. MII optimizes the actual model execution but leaves the pre/post processing pipeline for future optimizations. We notice that text representation tasks have significant pre/post processing overhead (*Figures G and H*). We plan to address those in a future update.

## Cost Sensitive Scenarios

MII can significantly reduce the inference cost of very expensive language models like Bloom, OPT, etc. To get the lowest cost, we use a large batch size that maximizes throughput for both baseline and MII. Here we look at the cost reduction from MII using two different metrics: i) tokens generated per second per GPU, and ii) dollars per million tokens generated.

*Figures I and J* show that MII with DeepSpeed-Generic offers over 10x throughput improvement and cost reduction compared to the baseline, respectively. Furthermore, running MII with DeepSpeed-Azure offers over 30x improvement in throughput and cost compared to baseline.

![tput large models](http://rasley.io/DeepSpeed/assets/images/mii/tput-llms.png){: .align-center}
*Figure I: Throughput comparison per A100-80GB GPU for large models*

![azure cost](http://rasley.io/DeepSpeed/assets/images/mii/azure-cost.png){: .align-center}
*Figure J: Cost of generating 1 million tokens on Azure with different model types*

# Deployment Options

MII supported models can be easily deployed with just a few lines of code in two different ways as shown in *Figure A*.

## MII-Local Deployment

MII can be deployed on-premises or on any cloud offering, with DeepSpeed-public as the backend. MII creates a lightweight GRPC server to support this form of deployment and provides a GRPC inference endpoint for queries. The code below shows how a supported model can be deployed with MII-Local Deployment.

```python
import mii
mii.deploy(task="text-to-image",
           model="CompVis/stable-diffusion-v1-4",
           deployment_name="sd-deployment",
           deployment_type=DeploymentType.LOCAL)
```

## MII-Azure Deployment

MII supports deployment on Azure via AML Inference. To enable this, MII generates AML deployment assets for a given model that can be deployed using the [Azure-CLI](https://learn.microsoft.com/en-us/cli/azure/what-is-azure-cli), as shown in the code below. Furthermore, deploying on Azure, allows MII to leverage DeepSpeed-Azure as its optimization backend, which offers better latency and cost reduction than DeepSpeed-Public.

```python
import mii
mii.deploy(task="text-to-image",
           model="CompVis/stable-diffusion-v1-4",
           deployment_name="sd-deployment",
           deployment_type=DeploymentType.AML)
```

To learn more about these deployment options and get started with MII, please the [MII getting started guide](https://github.com/microsoft/deepspeed-mii#getting-started-with-mii).

# Concluding Remarks

While open sourcing has made powerful AI capabilities accessible to many, MII allows for infusion of these capabilities into a diverse set of applications and product offerings by instantly reducing latency and cost of inferencing to our users though automatic application of an expansive set of inference optimizations from DeepSpeed.

**TODO:** add some text saying under development, updated more performant results across several model families are coming soon. Would be good (somewhere) to ensure the community we are committed to continued advancement of *both* public/azure sides.
