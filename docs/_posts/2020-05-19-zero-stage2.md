---
layout: single
title: "ZeRO-2 empowers training models as large as 170 billion parameters up to 10x faster compared to state-of-the-art"
excerpt: ""
categories: news
new_post: true
date: 2020-05-19 01:00:00
---

ZeRO-2 expands the scope of memory optimizations in the original ZeRO by
tackling the full spectrum of memory consumption during training. More
specifically, ZeRO-2 introduces new technology to reduce the memory footprint
of gradients, activation memory, and fragmented memory, in addition to
optimizer state memory optimization in the original ZeRO. Altogether, the
memory savings empower DeepSpeed to improve the scale and speed of deep
learning training by an order of magnitude. More concretely, ZeRO-2 allows
training models as large as 170 billion parameters up to 10x faster compared
to state of the art.

For more information on using ZeRO-2, see the [Megatron tutorial](/tutorials/megatron/).

For a technical deep dive, see our [technical report](https://arxiv.org/abs/1910.02054).
